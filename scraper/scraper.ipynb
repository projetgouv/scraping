{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[07:51:13] [DEBUG] - Opening the given URL\n",
      "[07:51:13] [DEBUG] - Accepting the cookies\n",
      "[07:51:19] [DEBUG] - Object_name OK : Újpesti 0-24 Gyógyszertár\n",
      "[07:51:19] [DEBUG] - Object_address OK : Budapest Attila utca 12-18 B épület földszint 4, 1047 Magyarország\n",
      "[07:51:19] [DEBUG] - Except branch\n",
      "[07:51:19] [DEBUG] - Overall_rating OK : 4,6\n",
      "[07:51:19] [DEBUG] - Review_number OK : 127\n",
      "[07:51:19] [DEBUG] - clicked to load further reviews\n",
      "[07:51:19] [DEBUG] - Scroll div OK\n",
      "[07:51:28] [ERROR] - https://www.google.com/maps/place/%C3%9Ajpesti+0-24+Gy%C3%B3gyszert%C3%A1r/@47.5644325,19.0890735,15.67z/data=!4m6!3m5!1s0x4741db39cf7c4a29:0xee59438ff2a16e76!8m2!3d47.5630657!4d19.0815841!16s%2Fg%2F11fvvjvtzh?hl=hu \n",
      " Message: no such window: target window already closed\n",
      "from unknown error: web view not found\n",
      "  (Session info: chrome=113.0.5672.63)\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x000000010513b8ac chromedriver + 4257964\n",
      "1   chromedriver                        0x0000000105133f40 chromedriver + 4226880\n",
      "2   chromedriver                        0x0000000104d709d4 chromedriver + 281044\n",
      "3   chromedriver                        0x0000000104d4adb0 chromedriver + 126384\n",
      "4   chromedriver                        0x0000000104dcf938 chromedriver + 670008\n",
      "5   chromedriver                        0x0000000104de1fe8 chromedriver + 745448\n",
      "6   chromedriver                        0x0000000104d9f98c chromedriver + 473484\n",
      "7   chromedriver                        0x0000000104da098c chromedriver + 477580\n",
      "8   chromedriver                        0x00000001050fa900 chromedriver + 3991808\n",
      "9   chromedriver                        0x00000001050fe354 chromedriver + 4006740\n",
      "10  chromedriver                        0x00000001050fe940 chromedriver + 4008256\n",
      "11  chromedriver                        0x000000010510433c chromedriver + 4031292\n",
      "12  chromedriver                        0x00000001050fef34 chromedriver + 4009780\n",
      "13  chromedriver                        0x00000001050d7490 chromedriver + 3847312\n",
      "14  chromedriver                        0x000000010511c9f4 chromedriver + 4131316\n",
      "15  chromedriver                        0x000000010511cb4c chromedriver + 4131660\n",
      "16  chromedriver                        0x000000010512d230 chromedriver + 4198960\n",
      "17  libsystem_pthread.dylib             0x000000019735606c _pthread_start + 148\n",
      "18  libsystem_pthread.dylib             0x0000000197350e2c thread_start + 8\n",
      "\n",
      "[07:51:28] [INFO] -  1 URL has been finished from the total of 1\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['name', 'date', 'rate', 'review_text', 'reply'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 275\u001b[0m\n\u001b[1;32m    271\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mSuccessfully exported the result file in the following folder: \u001b[39m\u001b[39m{\u001b[39;00mos\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(SAVING_PATH,\u001b[39m\"\u001b[39m\u001b[39mscrape_result.xlsx\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    274\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 275\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[1], line 259\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    250\u001b[0m result_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mjson_normalize(\n\u001b[1;32m    251\u001b[0m             scraped_data,\n\u001b[1;32m    252\u001b[0m             record_path \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mreviews\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m    253\u001b[0m             errors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    254\u001b[0m             meta\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mobject_name\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mobject_address\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39moverall_rating\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mreview_num\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mobject_url\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    255\u001b[0m             )\n\u001b[1;32m    258\u001b[0m \u001b[39m# reorder the columns\u001b[39;00m\n\u001b[0;32m--> 259\u001b[0m result_df \u001b[39m=\u001b[39m result_df[[\n\u001b[1;32m    260\u001b[0m             \u001b[39m'\u001b[39;49m\u001b[39mobject_name\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mobject_address\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39moverall_rating\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mreview_num\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m    261\u001b[0m             \u001b[39m'\u001b[39;49m\u001b[39mobject_url\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mname\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mdate\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mrate\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mreview_text\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mreply\u001b[39;49m\u001b[39m'\u001b[39;49m\n\u001b[1;32m    262\u001b[0m             ]]\n\u001b[1;32m    264\u001b[0m \u001b[39m# Saving the result into an excel file\u001b[39;00m\n\u001b[1;32m    265\u001b[0m save_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(SAVING_PATH,\u001b[39m'\u001b[39m\u001b[39mscrape_result.xlsx\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/repo/Hetic/projet_gouv/scraping/gouv_env/lib/python3.10/site-packages/pandas/core/frame.py:3767\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3765\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3766\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[0;32m-> 3767\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49m_get_indexer_strict(key, \u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39m]\n\u001b[1;32m   3769\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3770\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
      "File \u001b[0;32m~/repo/Hetic/projet_gouv/scraping/gouv_env/lib/python3.10/site-packages/pandas/core/indexes/base.py:5876\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5873\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   5874\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 5876\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   5878\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[1;32m   5879\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[1;32m   5880\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/repo/Hetic/projet_gouv/scraping/gouv_env/lib/python3.10/site-packages/pandas/core/indexes/base.py:5938\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5935\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   5937\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[0;32m-> 5938\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['name', 'date', 'rate', 'review_text', 'reply'] not in index\""
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "\n",
    "DRIVER_PATH = r'/home/oli/Projects/Google-review-scraper/chromedriver_linux64/chromedriver'\n",
    "SAVING_PATH = r'/home/oli/Projects/Google-review-scraper/data'\n",
    "\n",
    "# declaring a list, that contains the urls wich we want to be scraped\n",
    "OBJECT_URLS = [\n",
    "        #'https://www.google.com/maps/place/GoGo+hami+%C3%9Ajpest-k%C3%B6zpont/@47.6741137,18.6786254,11z/data=!4m7!3m6!1s0x4741da37020258d1:0xcac69f37622f45d0!8m2!3d47.5611747!4d19.0903816!15sCghnb2dvaGFtaVoKIghnb2dvaGFtaZIBCnJlc3RhdXJhbnTgAQA!16s%2Fg%2F11bwql9cb2?hl=hu&coh=164777&entry=tt&shorturl=1',\n",
    "        #'https://www.google.com/maps/place/Tesco/@47.7160014,18.7379746,16.04z/data=!4m6!3m5!1s0x476a645d4983b2df:0xf8f1eb25f3813b5b!8m2!3d47.7131753!4d18.7406381!16s%2Fg%2F1hg50d6_k?hl=hu',\n",
    "        'https://www.google.com/maps/place/%C3%9Ajpesti+0-24+Gy%C3%B3gyszert%C3%A1r/@47.5644325,19.0890735,15.67z/data=!4m6!3m5!1s0x4741db39cf7c4a29:0xee59438ff2a16e76!8m2!3d47.5630657!4d19.0815841!16s%2Fg%2F11fvvjvtzh?hl=hu',\n",
    "    ]\n",
    "\n",
    "# setting up the logging object\n",
    "logger = logging.getLogger('main')\n",
    "logging.basicConfig(\n",
    "    format='[%(asctime)s] [%(levelname)s] - %(message)s',\n",
    "    datefmt='%H:%M:%S'\n",
    "    )\n",
    "\n",
    "# we can change the logging level. Use logging.DEBUG if necesarry\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "def scrape_an_object(object_url: str) -> tuple :\n",
    "    \"\"\"\n",
    "    This method will:\n",
    "    - open the input URL (of a google maps object like stores, hotels, restaurants etc...)\n",
    "    - accept the cookies\n",
    "    - get some basic information of the given object (name, address, overall rating, \n",
    "      and the number of reviews)\n",
    "    - scroll down to the bottom of the page in order to load every reviews in the html source code\n",
    "    - scrape the div that contains the reviews\n",
    "\n",
    "    args: \n",
    "        object_url: the url of the google maps object to open\n",
    "    \n",
    "    returns a tuple containing:\n",
    "        store_main_data: a dictionary containing the basic information of the google map object \n",
    "                      (name, address, overall rating, and the number of reviews)\n",
    "\n",
    "        reviews_source: a bs4 object containing the html source code of the div \n",
    "                        that contains all the reviews\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # setting the chrome driver for selenium\n",
    "    driver = webdriver.Chrome(service=Service(DRIVER_PATH))\n",
    "\n",
    "    # opening the given URL\n",
    "    logger.debug(\"Opening the given URL\")\n",
    "    driver.get(object_url)\n",
    "    \n",
    "\n",
    "    # accepting the cookies\n",
    "    logger.debug(\"Accepting the cookies\")\n",
    "    driver.find_element(By.CLASS_NAME,\"lssxud\").click()\n",
    "\n",
    "    # waiting some random seconds\n",
    "    time.sleep(random.uniform(4,6))\n",
    "\n",
    "    # I use CSS selectors where I can, because its more robust than XPATH\n",
    "    object_name = driver.find_element(\n",
    "        By.CSS_SELECTOR,\n",
    "        'h1.DUwDvf.fontHeadlineLarge'\n",
    "    ).text\n",
    "    logger.debug(f'Object_name OK : {object_name}')\n",
    "\n",
    "    object_address = driver.find_element(\n",
    "        By.CSS_SELECTOR,\n",
    "        'div.Io6YTe.fontBodyMedium'\n",
    "    ).text\n",
    "    logger.debug(f'Object_address OK : {object_address}')\n",
    "\n",
    "\n",
    "    # for some reason sometimes google full randomly loads the page\n",
    "    # with a slightly different page structure. to be able to handle this,\n",
    "    # I created an except branch that scrapes the right objects in that scenario\n",
    "    try:\n",
    "\n",
    "        overall_rating = driver.find_element(\n",
    "            By.CSS_SELECTOR,\n",
    "            'div.F7nice.mmu3tf'\n",
    "        ).text.split()[0]\n",
    "        logger.debug(f'Overall_rating OK : {overall_rating}')\n",
    "\n",
    "        review_number = driver.find_element(\n",
    "            By.CSS_SELECTOR,\n",
    "            'div.F7nice.mmu3tf'\n",
    "        ).text.replace(' ','')\n",
    "\n",
    "        review_number = int(re.compile(r'\\d+').findall(review_number)[-1])\n",
    "        logger.debug(f'Review_number OK : {review_number}')\n",
    "\n",
    "        # click to load further reviews\n",
    "        driver.find_element(\n",
    "            By.XPATH,\n",
    "            '//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[2]/div[1]/div[1]/div[2]/div/div[1]/div[2]/span[2]/span[1]/span'\n",
    "        ).click()\n",
    "\n",
    "        logger.debug('Clicked to load further reviews')\n",
    "    \n",
    "        time.sleep(random.uniform(0.1, 0.5))\n",
    "\n",
    "        # find scroll layout\n",
    "        scrollable_div = driver.find_element(\n",
    "            By.XPATH,\n",
    "            '//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[2]'\n",
    "        )\n",
    "\n",
    "        logger.debug('Scroll div OK')\n",
    "     \n",
    "    except NoSuchElementException:\n",
    "\n",
    "        logger.debug('Except branch')\n",
    "\n",
    "        div_num_rating = driver.find_element(\n",
    "            By.CSS_SELECTOR,\n",
    "            'div.F7nice'\n",
    "        ).text\n",
    "        overall_rating = div_num_rating.split()[0]\n",
    "        logger.debug(f'Overall_rating OK : {overall_rating}')\n",
    "\n",
    "        review_number = int(div_num_rating.split()[1].replace('(','').replace(')',''))\n",
    "        logger.debug(f'Review_number OK : {review_number}')\n",
    "\n",
    "        # click on the review tab\n",
    "        driver.find_element(\n",
    "            By.XPATH,\n",
    "            '//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[3]/div/div/button[2]/div[2]/div[2]'\n",
    "        ).click()\n",
    "        logger.debug('clicked to load further reviews')\n",
    "\n",
    "        time.sleep(random.uniform(0.1, 0.5))\n",
    "\n",
    "        # find scroll layout\n",
    "        scrollable_div = driver.find_element(\n",
    "            By.XPATH,\n",
    "            '//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[3]'\n",
    "        )\n",
    "        logger.debug('Scroll div OK')\n",
    "\n",
    "    time.sleep(random.uniform(2,4))\n",
    "\n",
    "    # scroll as many times as necessary to load all reviews\n",
    "    for _ in range(0,(round(review_number/5 - 1)+1)):\n",
    "        driver.execute_script(\n",
    "            'arguments[0].scrollTop = arguments[0].scrollHeight',\n",
    "            scrollable_div\n",
    "        )\n",
    "        time.sleep(random.uniform(1, 2))\n",
    "\n",
    "    # parse the html with a bs object\n",
    "    response = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    reviews_source = response.find_all('div', class_='jJc9Ad')\n",
    "    logger.debug('Source code has been parsed!')\n",
    "\n",
    "    # closing the browser\n",
    "    driver.close()\n",
    "\n",
    "    # storing the data in a dict\n",
    "    store_main_data = {'object_name': object_name,\n",
    "                       'object_address': object_address,\n",
    "                       'overall_rating': overall_rating,\n",
    "                       'review_num': review_number,\n",
    "                       'object_url':object_url}\n",
    "\n",
    "    return store_main_data, reviews_source\n",
    "\n",
    "\n",
    "\n",
    "def extract_reviews(reviews_source: list) -> list:\n",
    "\n",
    "    r\"\"\"\n",
    "    This method processes the input html code and returns a list \n",
    "    containing the reviews.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    review_list = []\n",
    "\n",
    "    logger.debug('Starting iterate trough the reviews...')\n",
    "    for review in reviews_source:\n",
    "\n",
    "        # extract the relevant informations\n",
    "        user = review.find('div', class_= 'd4r55').text.strip()\n",
    "        date = review.find('span', class_= 'rsqaWe').text.strip()\n",
    "        rate = len(review.find('span',class_ = 'kvMYJc'))\n",
    "        review_text = review.find('span', class_= 'wiI7pd')\n",
    "        review_text = '' if review_text is None else review_text.text \n",
    "        reply_source = review.find('div', class_= 'CDe7pd')\n",
    "        reply = reply_source.text if reply_source else '-'\n",
    "\n",
    "\n",
    "        review_list.append({'name': user,\n",
    "                            'date': date,\n",
    "                            'rate': rate,\n",
    "                            'review_text': review_text,\n",
    "                            'reply': reply})\n",
    "\n",
    "    return review_list\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    scraped_data =  []\n",
    "\n",
    "    # loop trough the urls and calling the necessary functions to populate the empty scraped_data list\n",
    "    for i, url in enumerate(OBJECT_URLS):\n",
    "        try:\n",
    "            time.sleep(random.uniform(3,10))\n",
    "            \n",
    "            store_main_data, reviews_source = scrape_an_object(url)\n",
    "            scraped_data.append(store_main_data)\n",
    "\n",
    "            review_list = extract_reviews(reviews_source)\n",
    "            scraped_data[i]['reviews'] = review_list\n",
    "\n",
    "            if scraped_data[i]['review_num'] != len(scraped_data[i]['reviews']):\n",
    "                logger.warning(f'For some reason not all the reviews had been scraped for the following object: {store_main_data[\"object_name\"]}')\n",
    "\n",
    "\n",
    "        except Exception as exception:\n",
    "            logger.error(f'{url} \\n {exception}')\n",
    "            scraped_data.append(\n",
    "                    {'object_name': 'Error',\n",
    "                    'object_address': 'Error',\n",
    "                    'overall_rating': 'None',\n",
    "                    'review_num': 'None',\n",
    "                    'object_url':url,\n",
    "                    'reviews':[{}]\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        logger.info(f' {i+1} URL has been finished from the total of {len(OBJECT_URLS)}')\n",
    "\n",
    "\n",
    "    # reading the dict with pandas\n",
    "    result_df = pd.json_normalize(\n",
    "                scraped_data,\n",
    "                record_path = ['reviews'],\n",
    "                errors='ignore',\n",
    "                meta=['object_name', 'object_address', 'overall_rating', 'review_num', 'object_url']\n",
    "                )\n",
    "\n",
    "\n",
    "    # reorder the columns\n",
    "    result_df = result_df[[\n",
    "                'object_name','object_address','overall_rating','review_num',\n",
    "                'object_url', 'name','date','rate','review_text','reply'\n",
    "                ]]\n",
    "\n",
    "    # Saving the result into an excel file\n",
    "    save_path = os.path.join(SAVING_PATH,'scrape_result.xlsx')\n",
    "    result_df.to_excel(\n",
    "        save_path,\n",
    "        index= False\n",
    "    )\n",
    "\n",
    "    logger.info(f'Successfully exported the result file in the following folder: {os.path.join(SAVING_PATH,\"scrape_result.xlsx\")}')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gouv_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
