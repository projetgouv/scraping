{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code scraping fonctionnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv('/Users/camille/repo/Hetic/projet_gouv/scraping/Data/rec.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[00:10:50] [DEBUG] - Opening the given URL\n",
      "[00:10:50] [DEBUG] - Accepting the cookies\n",
      "[00:10:58] [DEBUG] - Object_name OK : Rectorat de l'acadÃ©mie de Lille\n",
      "[00:10:58] [DEBUG] - Object_address OK : 144 Rue de Bavay, 59000 Lille\n",
      "[00:10:58] [DEBUG] - Except branch\n",
      "[00:10:58] [DEBUG] - Overall_rating OK : 1.9\n",
      "[00:10:58] [DEBUG] - Review_number OK : 202\n",
      "[00:10:58] [DEBUG] - clicked to load further reviews\n",
      "[00:10:58] [DEBUG] - Scroll div OK\n"
     ]
    },
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=113.0.5672.126)\nStacktrace:\n0   chromedriver                        0x00000001053178ac chromedriver + 4257964\n1   chromedriver                        0x000000010530ff40 chromedriver + 4226880\n2   chromedriver                        0x0000000104f4c9d4 chromedriver + 281044\n3   chromedriver                        0x0000000104f26db0 chromedriver + 126384\n4   chromedriver                        0x0000000104fab938 chromedriver + 670008\n5   chromedriver                        0x0000000104fbdfe8 chromedriver + 745448\n6   chromedriver                        0x0000000104f7b98c chromedriver + 473484\n7   chromedriver                        0x0000000104f7c98c chromedriver + 477580\n8   chromedriver                        0x00000001052d6900 chromedriver + 3991808\n9   chromedriver                        0x00000001052da354 chromedriver + 4006740\n10  chromedriver                        0x00000001052da940 chromedriver + 4008256\n11  chromedriver                        0x00000001052e033c chromedriver + 4031292\n12  chromedriver                        0x00000001052daf34 chromedriver + 4009780\n13  chromedriver                        0x00000001052b3490 chromedriver + 3847312\n14  chromedriver                        0x00000001052f89f4 chromedriver + 4131316\n15  chromedriver                        0x00000001052f8b4c chromedriver + 4131660\n16  chromedriver                        0x0000000105309230 chromedriver + 4198960\n17  libsystem_pthread.dylib             0x0000000192b8a06c _pthread_start + 148\n18  libsystem_pthread.dylib             0x0000000192b84e2c thread_start + 8\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 226\u001b[0m\n\u001b[1;32m    223\u001b[0m     df\u001b[39m.\u001b[39mto_csv(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(SAVING_PATH, \u001b[39m'\u001b[39m\u001b[39mgoogle_reviews.csv\u001b[39m\u001b[39m'\u001b[39m), index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    225\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 226\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[9], line 214\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[39m# iterating through each object url in the list\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[39mfor\u001b[39;00m location \u001b[39min\u001b[39;00m df1[\u001b[39m'\u001b[39m\u001b[39mGM_search\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m--> 214\u001b[0m     store_main_data, reviews_source \u001b[39m=\u001b[39m scrape_an_object(OBJECT_URLS, location)\n\u001b[1;32m    215\u001b[0m     store_main_data \u001b[39m=\u001b[39m extract_reviews(reviews_source)\n\u001b[1;32m    216\u001b[0m     all_objects_data\u001b[39m.\u001b[39mappend(store_main_data)\n",
      "Cell \u001b[0;32mIn[9], line 139\u001b[0m, in \u001b[0;36mscrape_an_object\u001b[0;34m(object_url, location)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39m# scroll as many times as necessary to load all reviews\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m,(\u001b[39mround\u001b[39m(review_number\u001b[39m/\u001b[39m\u001b[39m5\u001b[39m \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)):\n\u001b[0;32m--> 139\u001b[0m     driver\u001b[39m.\u001b[39;49mexecute_script(\n\u001b[1;32m    140\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39marguments[0].scrollTop = arguments[0].scrollHeight\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m    141\u001b[0m         scrollable_div\n\u001b[1;32m    142\u001b[0m     )\n\u001b[1;32m    143\u001b[0m     \u001b[39m# click on 'more' botton if it appears\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/repo/Hetic/projet_gouv/scraping/gouv_env/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py:500\u001b[0m, in \u001b[0;36mWebDriver.execute_script\u001b[0;34m(self, script, *args)\u001b[0m\n\u001b[1;32m    497\u001b[0m converted_args \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(args)\n\u001b[1;32m    498\u001b[0m command \u001b[39m=\u001b[39m Command\u001b[39m.\u001b[39mW3C_EXECUTE_SCRIPT\n\u001b[0;32m--> 500\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(command, {\u001b[39m\"\u001b[39;49m\u001b[39mscript\u001b[39;49m\u001b[39m\"\u001b[39;49m: script, \u001b[39m\"\u001b[39;49m\u001b[39margs\u001b[39;49m\u001b[39m\"\u001b[39;49m: converted_args})[\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/repo/Hetic/projet_gouv/scraping/gouv_env/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py:440\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    438\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_executor\u001b[39m.\u001b[39mexecute(driver_command, params)\n\u001b[1;32m    439\u001b[0m \u001b[39mif\u001b[39;00m response:\n\u001b[0;32m--> 440\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror_handler\u001b[39m.\u001b[39;49mcheck_response(response)\n\u001b[1;32m    441\u001b[0m     response[\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unwrap_value(response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    442\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/repo/Hetic/projet_gouv/scraping/gouv_env/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py:245\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    243\u001b[0m         alert_text \u001b[39m=\u001b[39m value[\u001b[39m\"\u001b[39m\u001b[39malert\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    244\u001b[0m     \u001b[39mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[39m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[0;32m--> 245\u001b[0m \u001b[39mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[0;31mNoSuchWindowException\u001b[0m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=113.0.5672.126)\nStacktrace:\n0   chromedriver                        0x00000001053178ac chromedriver + 4257964\n1   chromedriver                        0x000000010530ff40 chromedriver + 4226880\n2   chromedriver                        0x0000000104f4c9d4 chromedriver + 281044\n3   chromedriver                        0x0000000104f26db0 chromedriver + 126384\n4   chromedriver                        0x0000000104fab938 chromedriver + 670008\n5   chromedriver                        0x0000000104fbdfe8 chromedriver + 745448\n6   chromedriver                        0x0000000104f7b98c chromedriver + 473484\n7   chromedriver                        0x0000000104f7c98c chromedriver + 477580\n8   chromedriver                        0x00000001052d6900 chromedriver + 3991808\n9   chromedriver                        0x00000001052da354 chromedriver + 4006740\n10  chromedriver                        0x00000001052da940 chromedriver + 4008256\n11  chromedriver                        0x00000001052e033c chromedriver + 4031292\n12  chromedriver                        0x00000001052daf34 chromedriver + 4009780\n13  chromedriver                        0x00000001052b3490 chromedriver + 3847312\n14  chromedriver                        0x00000001052f89f4 chromedriver + 4131316\n15  chromedriver                        0x00000001052f8b4c chromedriver + 4131660\n16  chromedriver                        0x0000000105309230 chromedriver + 4198960\n17  libsystem_pthread.dylib             0x0000000192b8a06c _pthread_start + 148\n18  libsystem_pthread.dylib             0x0000000192b84e2c thread_start + 8\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "\n",
    "DRIVER_PATH = r'/home/oli/Projects/Google-review-scraper/chromedriver_linux64/chromedriver'\n",
    "SAVING_PATH = '/Users/camille/repo/Hetic/projet_gouv/scraping/Data'\n",
    "\n",
    "# declaring a list, that contains the urls wich we want to be scraped\n",
    "OBJECT_URLS = \"https://www.google.com/maps/\"\n",
    "    \n",
    "\n",
    "# setting up the logging object\n",
    "logger = logging.getLogger('main')\n",
    "logging.basicConfig(\n",
    "    format='[%(asctime)s] [%(levelname)s] - %(message)s',\n",
    "    datefmt='%H:%M:%S'\n",
    "    )\n",
    "\n",
    "# we can change the logging level. Use logging.DEBUG if necesarry\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "def scrape_an_object(object_url, location):\n",
    "    # setting the chrome driver for selenium\n",
    "    driver = webdriver.Chrome(service=Service(DRIVER_PATH))\n",
    "\n",
    "    # opening the given URL\n",
    "    logger.debug(\"Opening the given URL\")\n",
    "    driver.get(object_url)\n",
    "\n",
    "    # accepting the cookies\n",
    "    logger.debug(\"Accepting the cookies\")\n",
    "    driver.find_element(By.CLASS_NAME,\"lssxud\").click()\n",
    "\n",
    "    # waiting some random seconds\n",
    "    time.sleep(random.uniform(4,6))\n",
    "    select_box = driver.find_element(By.XPATH, '//*[@id=\"searchboxinput\"]')\n",
    "    select_box.send_keys(location)\n",
    "    select_box.send_keys(Keys.ENTER)\n",
    "    time.sleep(2)\n",
    "    object_name = driver.find_element(\n",
    "    By.CSS_SELECTOR,\n",
    "    'h1.DUwDvf.fontHeadlineLarge'\n",
    "    ).text\n",
    "    logger.debug(f'Object_name OK : {object_name}')\n",
    "\n",
    "    object_address = driver.find_element(\n",
    "        By.CSS_SELECTOR,\n",
    "        'div.Io6YTe.fontBodyMedium'\n",
    "    ).text\n",
    "    logger.debug(f'Object_address OK : {object_address}')\n",
    "\n",
    "    # I use CSS selectors where I can, because its more robust than XPATH\n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "\n",
    "        overall_rating = driver.find_element(\n",
    "            By.CSS_SELECTOR,\n",
    "            'div.F7nice.mmu3tf'\n",
    "        ).text.split()[0]\n",
    "        logger.debug(f'Overall_rating OK : {overall_rating}')\n",
    "\n",
    "        review_number = driver.find_element(\n",
    "            By.CSS_SELECTOR,\n",
    "            'div.F7nice.mmu3tf'\n",
    "        ).text.replace(' ','')\n",
    "\n",
    "        review_number = int(re.compile(r'\\d+').findall(review_number)[-1])\n",
    "        logger.debug(f'Review_number OK : {review_number}')\n",
    "\n",
    "        # click to load further reviews\n",
    "        driver.find_element(\n",
    "            By.XPATH,\n",
    "            '//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[2]/div[1]/div[1]/div[2]/div/div[1]/div[2]/span[2]/span[1]/span'\n",
    "        ).click()\n",
    "\n",
    "        logger.debug('Clicked to load further reviews')\n",
    "    \n",
    "        time.sleep(random.uniform(0.1, 0.5))\n",
    "\n",
    "        # find scroll layout\n",
    "        scrollable_div = driver.find_element(\n",
    "            By.XPATH,\n",
    "            '//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[2]')\n",
    "    \n",
    "\n",
    "        logger.debug('Scroll div OK')\n",
    "\n",
    "\n",
    "    except NoSuchElementException:\n",
    "\n",
    "        logger.debug('Except branch')\n",
    "\n",
    "        div_num_rating = driver.find_element(\n",
    "            By.CSS_SELECTOR,\n",
    "            'div.F7nice'\n",
    "        ).text\n",
    "        overall_rating = div_num_rating.split()[0]\n",
    "        logger.debug(f'Overall_rating OK : {overall_rating}')\n",
    "\n",
    "        review_number = int(div_num_rating.split()[1].replace('(','').replace(')',''))\n",
    "        logger.debug(f'Review_number OK : {review_number}')\n",
    "\n",
    "        # click on the review tab\n",
    "        driver.find_element(By.XPATH,'/html/body/div[3]/div[9]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[3]/div[1]').click()\n",
    "        logger.debug('clicked to load further reviews')\n",
    "\n",
    "        time.sleep(random.uniform(0.1, 0.5))\n",
    "\n",
    "        # find scroll layout\n",
    "        scrollable_div = driver.find_element(\n",
    "            By.XPATH,\n",
    "            '//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[3]'\n",
    "        )\n",
    "        logger.debug('Scroll div OK')\n",
    "        #button = driver.element_to_be_clickable((By.CSS_SELECTOR, \"button.w8nwRe\"))\n",
    "        #button.click()\n",
    "\n",
    "\n",
    "    time.sleep(random.uniform(2,4))\n",
    "\n",
    "    # scroll as many times as necessary to load all reviews\n",
    "    for _ in range(0,(round(review_number/5 - 1)+1)):\n",
    "        driver.execute_script(\n",
    "            'arguments[0].scrollTop = arguments[0].scrollHeight',\n",
    "            scrollable_div\n",
    "        )\n",
    "        # click on 'more' botton if it appears\n",
    "        try:\n",
    "            button = driver.find_element(\n",
    "                By.CSS_SELECTOR,\n",
    "                'button.w8nwRe.kyuRq'\n",
    "            )\n",
    "            button.click()\n",
    "        except: \n",
    "            pass\n",
    "        time.sleep(random.uniform(1, 2))\n",
    "\n",
    "    # parse the html with a bs object\n",
    "    response = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    reviews_source = response.find_all('div', class_='jJc9Ad')\n",
    "    logger.debug('Source code has been parsed!')\n",
    "\n",
    "    # closing the browser\n",
    "    \n",
    "        \n",
    "    \n",
    "\n",
    "    # storing the data in a dict\n",
    "    store_main_data = {'object_name': object_name,\n",
    "                       'object_address': object_address,\n",
    "                       'overall_rating': overall_rating,\n",
    "                       'review_num': review_number,\n",
    "                       'object_url':object_url}\n",
    "\n",
    "    return store_main_data, reviews_source\n",
    "\n",
    "def extract_reviews(reviews_source: list) -> list:\n",
    "\n",
    "    r\"\"\"\n",
    "    This method processes the input html code and returns a list \n",
    "    containing the reviews.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    review_list = []\n",
    "\n",
    "    logger.debug('Starting iterate trough the reviews...')\n",
    "    for review in reviews_source:\n",
    "\n",
    "        # extract the relevant informations\n",
    "        user = review.find('div', class_= 'd4r55').text.strip()\n",
    "        date = review.find('span', class_= 'rsqaWe').text.strip()\n",
    "        rate = len(review.find('span',class_ = 'kvMYJc'))\n",
    "        review_text = review.find('span', class_= 'wiI7pd')\n",
    "        review_text = '' if review_text is None else review_text.text \n",
    "        reply_source = review.find('div', class_= 'CDe7pd')\n",
    "        reply = reply_source.text if reply_source else '-'\n",
    "\n",
    "\n",
    "        review_list.append({'name': user,\n",
    "                            'date': date,\n",
    "                            'rate': rate,\n",
    "                            'review_text': review_text,\n",
    "                            'reply': reply})\n",
    "\n",
    "    return review_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # creating a list to store all objects scraped\n",
    "    all_objects_data = []\n",
    "\n",
    "    # iterating through each object url in the list\n",
    "    for location in df1['GM_search']:\n",
    "        store_main_data, reviews_source = scrape_an_object(OBJECT_URLS, location)\n",
    "        store_main_data = extract_reviews(reviews_source)\n",
    "        all_objects_data.append(store_main_data)\n",
    "        logger.debug(f'{location} is done!')\n",
    "\n",
    "    # creating a dataframe from the list of dictionaries\n",
    "    df = pd.DataFrame(all_objects_data)\n",
    "\n",
    "    # writing the dataframe to a csv file\n",
    "    df.to_csv(os.path.join(SAVING_PATH, 'google_reviews.csv'), index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gouv_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
