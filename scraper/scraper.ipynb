{
 "cells": [
  {
<<<<<<< HEAD
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:39:02] [DEBUG] - Opening the given URL\n",
      "[23:39:06] [DEBUG] - Accepting the cookies\n",
      "[23:39:18] [DEBUG] - Object_name OK : Rectorat de l'acadÃ©mie de Lille\n",
      "[23:39:18] [DEBUG] - Object_address OK : 144 Rue de Bavay, 59000 Lille\n",
      "[23:39:18] [DEBUG] - Except branch\n",
      "[23:39:18] [DEBUG] - Overall_rating OK : 1,9\n",
      "[23:39:18] [DEBUG] - Review_number OK : 201\n",
      "[23:39:19] [DEBUG] - clicked to load further reviews\n",
      "[23:39:19] [DEBUG] - Scroll div OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<selenium.webdriver.remote.webelement.WebElement (session=\"46a9a40547f8676ca277f913e08f7992\", element=\"E9E30A8DD21578685981C8445982D437_element_83\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"46a9a40547f8676ca277f913e08f7992\", element=\"E9E30A8DD21578685981C8445982D437_element_84\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"46a9a40547f8676ca277f913e08f7992\", element=\"E9E30A8DD21578685981C8445982D437_element_85\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"46a9a40547f8676ca277f913e08f7992\", element=\"E9E30A8DD21578685981C8445982D437_element_86\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"46a9a40547f8676ca277f913e08f7992\", element=\"E9E30A8DD21578685981C8445982D437_element_87\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"46a9a40547f8676ca277f913e08f7992\", element=\"E9E30A8DD21578685981C8445982D437_element_88\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"46a9a40547f8676ca277f913e08f7992\", element=\"E9E30A8DD21578685981C8445982D437_element_89\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"46a9a40547f8676ca277f913e08f7992\", element=\"E9E30A8DD21578685981C8445982D437_element_90\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"46a9a40547f8676ca277f913e08f7992\", element=\"E9E30A8DD21578685981C8445982D437_element_91\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"46a9a40547f8676ca277f913e08f7992\", element=\"E9E30A8DD21578685981C8445982D437_element_92\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"46a9a40547f8676ca277f913e08f7992\", element=\"E9E30A8DD21578685981C8445982D437_element_93\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"46a9a40547f8676ca277f913e08f7992\", element=\"E9E30A8DD21578685981C8445982D437_element_94\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"46a9a40547f8676ca277f913e08f7992\", element=\"E9E30A8DD21578685981C8445982D437_element_95\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"46a9a40547f8676ca277f913e08f7992\", element=\"E9E30A8DD21578685981C8445982D437_element_96\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"46a9a40547f8676ca277f913e08f7992\", element=\"E9E30A8DD21578685981C8445982D437_element_97\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"46a9a40547f8676ca277f913e08f7992\", element=\"E9E30A8DD21578685981C8445982D437_element_98\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"46a9a40547f8676ca277f913e08f7992\", element=\"E9E30A8DD21578685981C8445982D437_element_99\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"46a9a40547f8676ca277f913e08f7992\", element=\"E9E30A8DD21578685981C8445982D437_element_100\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"46a9a40547f8676ca277f913e08f7992\", element=\"E9E30A8DD21578685981C8445982D437_element_101\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"46a9a40547f8676ca277f913e08f7992\", element=\"E9E30A8DD21578685981C8445982D437_element_102\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"46a9a40547f8676ca277f913e08f7992\", element=\"E9E30A8DD21578685981C8445982D437_element_103\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"46a9a40547f8676ca277f913e08f7992\", element=\"E9E30A8DD21578685981C8445982D437_element_104\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"46a9a40547f8676ca277f913e08f7992\", element=\"E9E30A8DD21578685981C8445982D437_element_105\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"46a9a40547f8676ca277f913e08f7992\", element=\"E9E30A8DD21578685981C8445982D437_element_106\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"46a9a40547f8676ca277f913e08f7992\", element=\"E9E30A8DD21578685981C8445982D437_element_107\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"46a9a40547f8676ca277f913e08f7992\", element=\"E9E30A8DD21578685981C8445982D437_element_108\")>]\n",
      "je clicke\n",
      "je clicke\n",
      "je clicke\n",
      "je clicke\n",
      "je clicke\n",
      "je clicke\n",
      "je clicke\n",
      "je clicke\n",
      "je clicke\n",
      "je clicke\n",
      "je clicke\n",
      "je clicke\n",
      "je clicke\n",
      "je clicke\n",
      "je clicke\n",
      "je clicke\n",
      "je clicke\n",
      "je clicke\n",
      "je clicke\n",
      "je clicke\n",
      "je clicke\n",
      "je clicke\n",
      "je clicke\n",
      "je clicke\n",
      "je clicke\n",
      "je clicke\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:40:41] [DEBUG] - Source code has been parsed!\n",
      "[23:40:45] [DEBUG] - Starting iterate trough the reviews...\n",
      "[23:40:45] [INFO] -  1 URL has been finished from the total of 1\n",
      "[23:40:45] [INFO] - Successfully exported the result file in the following folder: C:\\Users\\lucie\\OneDrive\\Documents\\dependance\\scraping\\scrape_result.csv\n",
      "[23:40:45] [INFO] - Finished!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201 201\n"
     ]
    }
   ],
=======
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code scraping fonctionnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv('Data/rec.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 99bc2c43cf5325e83255c03cbefb613ce00031dd
   "source": [
    "import time\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
<<<<<<< HEAD
    "from selenium.webdriver.support import expected_conditions as EC\n",
=======
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
>>>>>>> 99bc2c43cf5325e83255c03cbefb613ce00031dd
    "\n",
    "\n",
    "DRIVER_PATH = r'C:\\Users\\lucie\\OneDrive\\Documents\\dependance\\chromedriver_win32\\chromedriver'\n",
    "SAVING_PATH = r'C:\\Users\\lucie\\OneDrive\\Documents\\dependance\\scraping'\n",
    "\n",
    "# declaring a list, that contains the urls wich we want to be scraped\n",
<<<<<<< HEAD
    "OBJECT_URLS = [ \"https://www.google.com/maps/place/Rectorat+de+l'acad%C3%A9mie+de+Lille/@50.6261288,3.0786046,17z/data=!3m1!4b1!4m6!3m5!1s0x47c32a77b52d7745:0x6571dfb526d328d9!8m2!3d50.6261254!4d3.0811795!16s%2Fg%2F1tvq38jm\",\n",
    "               ]\n",
=======
    "OBJECT_URLS = \"https://www.google.com/maps/\"\n",
    "    \n",
>>>>>>> 99bc2c43cf5325e83255c03cbefb613ce00031dd
    "\n",
    "# setting up the logging object\n",
    "logger = logging.getLogger('main')\n",
    "logging.basicConfig(\n",
    "    format='[%(asctime)s] [%(levelname)s] - %(message)s',\n",
    "    datefmt='%H:%M:%S'\n",
    "    )\n",
    "\n",
    "# we can change the logging level. Use logging.DEBUG if necesarry\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
<<<<<<< HEAD
    "def scrape_an_object(object_url: str) -> tuple :\n",
    "\n",
    "\n",
=======
    "def scrape_an_object(object_url, location):\n",
>>>>>>> 99bc2c43cf5325e83255c03cbefb613ce00031dd
    "    # setting the chrome driver for selenium\n",
    "    driver = webdriver.Chrome(service=Service(DRIVER_PATH))\n",
    "\n",
    "    # opening the given URL\n",
    "    logger.debug(\"Opening the given URL\")\n",
    "    driver.get(object_url)\n",
    "\n",
    "    # accepting the cookies\n",
    "    logger.debug(\"Accepting the cookies\")\n",
    "    driver.find_element(By.CLASS_NAME,\"lssxud\").click()\n",
    "\n",
    "    # waiting some random seconds\n",
    "    time.sleep(random.uniform(4,6))\n",
    "    select_box = driver.find_element(By.XPATH, '//*[@id=\"searchboxinput\"]')\n",
    "    select_box.send_keys(location)\n",
    "    select_box.send_keys(Keys.ENTER)\n",
    "    time.sleep(2)\n",
    "    object_name = driver.find_element(\n",
    "    By.CSS_SELECTOR,\n",
    "    'h1.DUwDvf.fontHeadlineLarge'\n",
    "    ).text\n",
    "    logger.debug(f'Object_name OK : {object_name}')\n",
    "\n",
    "    object_address = driver.find_element(\n",
    "        By.CSS_SELECTOR,\n",
    "        'div.Io6YTe.fontBodyMedium'\n",
    "    ).text\n",
    "    logger.debug(f'Object_address OK : {object_address}')\n",
    "\n",
    "    # I use CSS selectors where I can, because its more robust than XPATH\n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "\n",
    "        overall_rating = driver.find_element(\n",
    "            By.CSS_SELECTOR,\n",
    "            'div.F7nice.mmu3tf'\n",
    "        ).text.split()[0]\n",
    "        logger.debug(f'Overall_rating OK : {overall_rating}')\n",
    "\n",
    "        review_number = driver.find_element(\n",
    "            By.CSS_SELECTOR,\n",
    "            'div.F7nice.mmu3tf'\n",
    "        ).text.replace(' ','')\n",
    "\n",
    "        review_number = int(re.compile(r'\\d+').findall(review_number)[-1])\n",
    "        logger.debug(f'Review_number OK : {review_number}')\n",
    "\n",
    "        # click to load further reviews\n",
    "        driver.find_element(\n",
    "            By.XPATH,\n",
    "            '//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[2]/div[1]/div[1]/div[2]/div/div[1]/div[2]/span[2]/span[1]/span'\n",
    "        ).click()\n",
    "\n",
    "        logger.debug('Clicked to load further reviews')\n",
    "    \n",
    "        time.sleep(random.uniform(0.1, 0.5))\n",
    "\n",
    "        # find scroll layout\n",
    "        scrollable_div = driver.find_element(\n",
    "            By.XPATH,\n",
    "            '//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[2]')\n",
    "    \n",
    "\n",
    "        logger.debug('Scroll div OK')\n",
    "\n",
    "\n",
    "    except NoSuchElementException:\n",
    "\n",
    "        logger.debug('Except branch')\n",
    "\n",
    "        div_num_rating = driver.find_element(\n",
    "            By.CSS_SELECTOR,\n",
    "            'div.F7nice'\n",
    "        ).text\n",
    "        overall_rating = div_num_rating.split()[0]\n",
    "        logger.debug(f'Overall_rating OK : {overall_rating}')\n",
    "\n",
    "        review_number = int(div_num_rating.split()[1].replace('(','').replace(')',''))\n",
    "        logger.debug(f'Review_number OK : {review_number}')\n",
    "\n",
    "        # click on the review tab\n",
    "        driver.find_element(By.XPATH,'/html/body/div[3]/div[9]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[3]/div[1]').click()\n",
    "        logger.debug('clicked to load further reviews')\n",
    "\n",
    "        time.sleep(random.uniform(0.1, 0.5))\n",
    "\n",
    "        # find scroll layout\n",
    "        scrollable_div = driver.find_element(\n",
    "            By.XPATH,\n",
    "            '//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[3]'\n",
    "        )\n",
    "\n",
    "        logger.debug('Scroll div OK')\n",
    "        #button = driver.element_to_be_clickable((By.CSS_SELECTOR, \"button.w8nwRe\"))\n",
    "        #button.click()\n",
    "\n",
    "\n",
    "    time.sleep(random.uniform(2,4))\n",
    "\n",
    "\n",
    "\n",
    "    # scroll as many times as necessary to load all reviews\n",
    "    for _ in range(0,(round(review_number/5 - 1)+1)):\n",
    "        driver.execute_script(\n",
    "            'arguments[0].scrollTop = arguments[0].scrollHeight',\n",
    "            scrollable_div\n",
    "        )\n",
    "        # click on 'more' botton if it appears\n",
    "        try:\n",
    "            button = driver.find_element(\n",
    "                By.CSS_SELECTOR,\n",
    "                'button.w8nwRe.kyuRq'\n",
    "            )\n",
    "            button.click()\n",
    "        except: \n",
    "            pass\n",
    "        time.sleep(random.uniform(1, 2))\n",
    "\n",
    "    # button lire plus\n",
    "    button_lire_plus = driver.find_elements(By.CLASS_NAME,'w8nwRe.kyuRq')\n",
    "    for i in button_lire_plus:\n",
    "        i.click()\n",
    "\n",
    "\n",
    "    # parse the html with a bs object\n",
    "    response = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    reviews_source = response.find_all('div', class_='jJc9Ad')\n",
    "    logger.debug('Source code has been parsed!')\n",
    "\n",
    "    # closing the browser\n",
    "    \n",
    "        \n",
    "    \n",
    "\n",
    "    # storing the data in a dict\n",
    "    store_main_data = {'object_name': object_name,\n",
    "                       'object_address': object_address,\n",
    "                       'overall_rating': overall_rating,\n",
    "                       'review_num': review_number,\n",
    "                       'object_url':object_url}\n",
    "\n",
    "    return store_main_data, reviews_source\n",
    "\n",
    "def extract_reviews(reviews_source: list) -> list:\n",
    "\n",
    "    r\"\"\"\n",
    "    This method processes the input html code and returns a list \n",
    "    containing the reviews.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    review_list = []\n",
    "\n",
    "    logger.debug('Starting iterate trough the reviews...')\n",
    "    for review in reviews_source:\n",
    "\n",
    "        # extract the relevant informations\n",
    "        user = review.find('div', class_= 'd4r55').text.strip()\n",
    "        date = review.find('span', class_= 'rsqaWe').text.strip()\n",
    "        rate = len(review.find('span',class_ = 'kvMYJc'))\n",
    "        review_text = review.find('span', class_= 'wiI7pd')\n",
    "        review_text = '' if review_text is None else review_text.text \n",
    "        reply_source = review.find('div', class_= 'CDe7pd')\n",
    "        reply = reply_source.text if reply_source else '-'\n",
    "\n",
    "\n",
    "        review_list.append({'name': user,\n",
    "                            'date': date,\n",
    "                            'rate': rate,\n",
    "                            'review_text': review_text,\n",
    "                            'reply': reply})\n",
    "\n",
    "    return review_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # creating a list to store all objects scraped\n",
    "    all_objects_data = []\n",
    "\n",
    "    # iterating through each object url in the list\n",
    "    for location in df1['GM_s   earch']:\n",
    "        store_main_data, reviews_source = scrape_an_object(OBJECT_URLS, location)\n",
    "        store_main_data = extract_reviews(reviews_source)\n",
    "        all_objects_data.append(store_main_data)\n",
    "        logger.debug(f'{location} is done!')\n",
    "\n",
    "    # creating a dataframe from the list of dictionaries\n",
    "    df = pd.DataFrame(all_objects_data)\n",
    "\n",
    "    # writing the dataframe to a csv file\n",
    "    df.to_csv(os.path.join(SAVING_PATH, 'google_reviews.csv'), index=False)\n",
    "\n",
<<<<<<< HEAD
    "\n",
    "        except Exception as exception:\n",
    "            logger.error(f'{url} \\n {exception}')\n",
    "            scraped_data.append(\n",
    "                    {'object_name': 'Error',\n",
    "                    'object_address': 'Error',\n",
    "                    'overall_rating': 'None',\n",
    "                    'review_num': 'None',\n",
    "                    'object_url':url,\n",
    "                    'reviews':[{}]\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        logger.info(f' {i+1} URL has been finished from the total of {len(OBJECT_URLS)}')\n",
    "\n",
    "\n",
    "    # reading the dict with pandas\n",
    "    result_df = pd.json_normalize(\n",
    "                scraped_data,\n",
    "                record_path = ['reviews'],\n",
    "                errors='ignore',\n",
    "                meta=['object_name', 'object_address', 'overall_rating', 'review_num', 'object_url']\n",
    "                )\n",
    "\n",
    "\n",
    "    # reorder the columns\n",
    "\n",
    "\n",
    "    # Saving the result into an excel file\n",
    "    save_path = os.path.join(SAVING_PATH,'scrape_result.csv')\n",
    "    result_df.to_csv(\n",
    "        save_path,\n",
    "        index= False\n",
    "    )\n",
    "\n",
    "    logger.info(f'Successfully exported the result file in the following folder: {os.path.join(SAVING_PATH,\"scrape_result.csv\")}')\n",
    "    logger.info('Finished!')\n",
=======
>>>>>>> 99bc2c43cf5325e83255c03cbefb613ce00031dd
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gouv_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
