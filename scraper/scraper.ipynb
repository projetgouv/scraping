{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code scraping fonctionnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_pe = pd.read_csv('/Users/camille/repo/Hetic/projet_gouv/scraping/Data/location.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:14:08] [DEBUG] - Opening the given URL\n",
      "[11:14:08] [DEBUG] - Accepting the cookies\n",
      "[11:14:15] [DEBUG] - Object_name OK : Pôle emploi\n",
      "[11:14:15] [DEBUG] - Object_address OK : 27 Rue Daviel, 75013 Paris\n",
      "[11:14:15] [DEBUG] - Except branch\n",
      "[11:14:15] [DEBUG] - Overall_rating OK : 2.5\n",
      "[11:14:15] [DEBUG] - Review_number OK : 82\n",
      "[11:14:15] [DEBUG] - clicked to load further reviews\n",
      "[11:14:16] [DEBUG] - Scroll div OK\n",
      "[11:14:31] [DEBUG] - Source code has been parsed!\n",
      "[11:14:31] [DEBUG] - Starting to iterate through the reviews...\n",
      "[11:14:31] [DEBUG] - Pole emploi Daviel 27 RUE Daviel 75013 Paris is done!\n",
      "[11:14:31] [DEBUG] - CSV file saved at: /Users/camille/repo/Hetic/projet_gouv/scraping/Data/google_reviews_Pole emploi Daviel 27 RUE Daviel 75013 Paris.csv\n",
      "[11:14:33] [DEBUG] - Opening the given URL\n",
      "[11:14:33] [DEBUG] - Accepting the cookies\n",
      "[11:14:41] [DEBUG] - Object_name OK : Pôle emploi\n",
      "[11:14:41] [DEBUG] - Object_address OK : 75 Av. Jean Jaurès, 75019 Paris\n",
      "[11:14:41] [DEBUG] - Except branch\n",
      "[11:14:41] [DEBUG] - Overall_rating OK : 3.2\n",
      "[11:14:41] [DEBUG] - Review_number OK : 25\n",
      "[11:14:41] [DEBUG] - clicked to load further reviews\n",
      "[11:14:42] [DEBUG] - Scroll div OK\n",
      "[11:14:49] [DEBUG] - Source code has been parsed!\n",
      "[11:14:49] [DEBUG] - Starting to iterate through the reviews...\n",
      "[11:14:49] [DEBUG] - Pole emploi Laumière 75 AVENUE JEAN JAURES 75019 Paris is done!\n",
      "[11:14:49] [DEBUG] - CSV file saved at: /Users/camille/repo/Hetic/projet_gouv/scraping/Data/google_reviews_Pole emploi Laumière 75 AVENUE JEAN JAURES 75019 Paris.csv\n",
      "[11:14:50] [DEBUG] - Opening the given URL\n",
      "[11:14:51] [DEBUG] - Accepting the cookies\n",
      "[11:14:59] [DEBUG] - Object_name OK : Pôle emploi\n",
      "[11:14:59] [DEBUG] - Object_address OK : 11 Rue Maurice Genevoix, 75018 Paris\n",
      "[11:14:59] [DEBUG] - Except branch\n",
      "[11:14:59] [DEBUG] - Overall_rating OK : 3.0\n",
      "[11:14:59] [DEBUG] - Review_number OK : 29\n",
      "[11:14:59] [DEBUG] - clicked to load further reviews\n",
      "[11:14:59] [DEBUG] - Scroll div OK\n",
      "[11:15:08] [DEBUG] - Source code has been parsed!\n",
      "[11:15:08] [DEBUG] - Starting to iterate through the reviews...\n",
      "[11:15:08] [DEBUG] - Pole emploi Genevoix 11 RUE Maurice Genevoix 75018 Paris is done!\n",
      "[11:15:08] [DEBUG] - CSV file saved at: /Users/camille/repo/Hetic/projet_gouv/scraping/Data/google_reviews_Pole emploi Genevoix 11 RUE Maurice Genevoix 75018 Paris.csv\n",
      "[11:15:09] [DEBUG] - Opening the given URL\n",
      "[11:15:10] [DEBUG] - Accepting the cookies\n",
      "[11:15:17] [DEBUG] - Object_name OK : Pôle emploi\n",
      "[11:15:17] [DEBUG] - Object_address OK : 3 Bd Diderot, 75012 Paris\n",
      "[11:15:17] [DEBUG] - Except branch\n",
      "[11:15:17] [DEBUG] - Overall_rating OK : 2.1\n",
      "[11:15:17] [DEBUG] - Review_number OK : 54\n",
      "[11:15:17] [DEBUG] - clicked to load further reviews\n",
      "[11:15:17] [DEBUG] - Scroll div OK\n",
      "[11:15:28] [DEBUG] - Source code has been parsed!\n",
      "[11:15:28] [DEBUG] - Starting to iterate through the reviews...\n",
      "[11:15:28] [DEBUG] - Pole emploi Diderot 3 BOULEVARD Diderot 75012 Paris is done!\n",
      "[11:15:28] [DEBUG] - CSV file saved at: /Users/camille/repo/Hetic/projet_gouv/scraping/Data/google_reviews_Pole emploi Diderot 3 BOULEVARD Diderot 75012 Paris.csv\n",
      "[11:15:30] [DEBUG] - Opening the given URL\n",
      "[11:15:30] [DEBUG] - Accepting the cookies\n",
      "[11:15:37] [DEBUG] - Object_name OK : Pôle emploi\n",
      "[11:15:37] [DEBUG] - Object_address OK : 4 Rue Paul Lelong, 75002 Paris\n",
      "[11:15:37] [DEBUG] - Except branch\n",
      "[11:15:37] [DEBUG] - Overall_rating OK : 2.6\n",
      "[11:15:37] [DEBUG] - Review_number OK : 28\n",
      "[11:15:38] [DEBUG] - clicked to load further reviews\n",
      "[11:15:38] [DEBUG] - Scroll div OK\n",
      "[11:15:45] [DEBUG] - Source code has been parsed!\n",
      "[11:15:45] [DEBUG] - Starting to iterate through the reviews...\n",
      "[11:15:45] [DEBUG] - Pole emploi Paul Lelong 4 RUE Paul Lelong 75002 Paris is done!\n",
      "[11:15:45] [DEBUG] - CSV file saved at: /Users/camille/repo/Hetic/projet_gouv/scraping/Data/google_reviews_Pole emploi Paul Lelong 4 RUE Paul Lelong 75002 Paris.csv\n",
      "[11:15:46] [DEBUG] - Opening the given URL\n",
      "[11:15:47] [DEBUG] - Accepting the cookies\n",
      "[11:15:54] [DEBUG] - Object_name OK : Pôle emploi\n",
      "[11:15:54] [DEBUG] - Object_address OK : 26 Rue Vicq d'Azir, 75010 Paris\n",
      "[11:15:54] [DEBUG] - Except branch\n",
      "[11:15:54] [DEBUG] - Overall_rating OK : 2.7\n",
      "[11:15:54] [DEBUG] - Review_number OK : 31\n",
      "[11:15:54] [DEBUG] - clicked to load further reviews\n",
      "[11:15:55] [DEBUG] - Scroll div OK\n",
      "[11:16:05] [DEBUG] - Source code has been parsed!\n",
      "[11:16:05] [DEBUG] - Starting to iterate through the reviews...\n",
      "[11:16:05] [DEBUG] - Pole emploi Vicq d'Azir 26 RUE Vicq d'Azir 75010 Paris is done!\n",
      "[11:16:05] [DEBUG] - CSV file saved at: /Users/camille/repo/Hetic/projet_gouv/scraping/Data/google_reviews_Pole emploi Vicq d'Azir 26 RUE Vicq d'Azir 75010 Paris.csv\n",
      "[11:16:07] [DEBUG] - Opening the given URL\n",
      "[11:16:07] [DEBUG] - Accepting the cookies\n",
      "[11:16:15] [DEBUG] - Object_name OK : Pôle emploi\n",
      "[11:16:15] [DEBUG] - Object_address OK : 10 Rue Brancion, 75015 Paris\n",
      "[11:16:15] [DEBUG] - Except branch\n",
      "[11:16:15] [DEBUG] - Overall_rating OK : 2.8\n",
      "[11:16:15] [DEBUG] - Review_number OK : 62\n",
      "[11:16:15] [DEBUG] - clicked to load further reviews\n",
      "[11:16:15] [DEBUG] - Scroll div OK\n",
      "[11:16:28] [DEBUG] - Source code has been parsed!\n",
      "[11:16:28] [DEBUG] - Starting to iterate through the reviews...\n",
      "[11:16:28] [DEBUG] - Pole emploi Brancion 10 RUE Brancion 75015 Paris is done!\n",
      "[11:16:28] [DEBUG] - CSV file saved at: /Users/camille/repo/Hetic/projet_gouv/scraping/Data/google_reviews_Pole emploi Brancion 10 RUE Brancion 75015 Paris.csv\n",
      "[11:16:30] [DEBUG] - Opening the given URL\n",
      "[11:16:30] [DEBUG] - Accepting the cookies\n",
      "[11:16:37] [DEBUG] - Object_name OK : Pôle emploi\n",
      "[11:16:38] [DEBUG] - Object_address OK : 78 Bd Ney, 75018 Paris\n",
      "[11:16:38] [DEBUG] - Except branch\n",
      "[11:16:38] [DEBUG] - Overall_rating OK : 2.3\n",
      "[11:16:38] [DEBUG] - Review_number OK : 54\n",
      "[11:16:38] [DEBUG] - clicked to load further reviews\n",
      "[11:16:38] [DEBUG] - Scroll div OK\n",
      "[11:16:50] [DEBUG] - Source code has been parsed!\n",
      "[11:16:50] [DEBUG] - Starting to iterate through the reviews...\n",
      "[11:16:50] [DEBUG] - Pole emploi Ney 78 BOULEVARD Ney 75018 PARIS is done!\n",
      "[11:16:50] [DEBUG] - CSV file saved at: /Users/camille/repo/Hetic/projet_gouv/scraping/Data/google_reviews_Pole emploi Ney 78 BOULEVARD Ney 75018 PARIS.csv\n",
      "[11:16:51] [DEBUG] - Opening the given URL\n",
      "[11:16:52] [DEBUG] - Accepting the cookies\n",
      "[11:16:58] [DEBUG] - Object_name OK : Pôle emploi - Paris 20ème Piat\n",
      "[11:16:58] [DEBUG] - Object_address OK : 51 Rue Piat, 75020 Paris\n",
      "[11:16:58] [DEBUG] - Except branch\n",
      "[11:16:58] [DEBUG] - Overall_rating OK : 3.0\n",
      "[11:16:58] [DEBUG] - Review_number OK : 20\n",
      "[11:16:58] [DEBUG] - clicked to load further reviews\n",
      "[11:16:59] [DEBUG] - Scroll div OK\n",
      "[11:17:05] [DEBUG] - Source code has been parsed!\n",
      "[11:17:05] [DEBUG] - Starting to iterate through the reviews...\n",
      "[11:17:05] [DEBUG] - Pôle emploi - Paris 20ème Piat is done!\n",
      "[11:17:05] [DEBUG] - CSV file saved at: /Users/camille/repo/Hetic/projet_gouv/scraping/Data/google_reviews_Pôle emploi - Paris 20ème Piat.csv\n",
      "[11:17:06] [DEBUG] - Opening the given URL\n",
      "[11:17:07] [DEBUG] - Accepting the cookies\n",
      "[11:17:14] [DEBUG] - Object_name OK : Pôle emploi\n",
      "[11:17:14] [DEBUG] - Object_address OK : 9 Rue Friant, 75014 Paris\n",
      "[11:17:14] [DEBUG] - Except branch\n",
      "[11:17:14] [DEBUG] - Overall_rating OK : 2.8\n",
      "[11:17:14] [DEBUG] - Review_number OK : 40\n",
      "[11:17:14] [DEBUG] - clicked to load further reviews\n",
      "[11:17:15] [DEBUG] - Scroll div OK\n",
      "[11:17:23] [DEBUG] - Source code has been parsed!\n",
      "[11:17:23] [DEBUG] - Starting to iterate through the reviews...\n",
      "[11:17:23] [DEBUG] - Pole emploi Jean Moulin 9 RUE FRIANT 75014 PARIS is done!\n",
      "[11:17:23] [DEBUG] - CSV file saved at: /Users/camille/repo/Hetic/projet_gouv/scraping/Data/google_reviews_Pole emploi Jean Moulin 9 RUE FRIANT 75014 PARIS.csv\n",
      "[11:17:23] [DEBUG] - All objects CSV file saved at: /Users/camille/repo/Hetic/projet_gouv/scraping/Data/google_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "DRIVER_PATH = r'/home/oli/Projects/Google-review-scraper/chromedriver_linux64/chromedriver'\n",
    "SAVING_PATH = '/Users/camille/repo/Hetic/projet_gouv/scraping/Data'\n",
    "\n",
    "# declaring a list, that contains the urls which we want to be scraped\n",
    "OBJECT_URLS = \"https://www.google.com/maps/\"\n",
    "df_pe = pd.read_csv('/Users/camille/repo/Hetic/projet_gouv/scraping/Data/location.csv', sep=';') \n",
    "\n",
    "# setting up the logging object\n",
    "logger = logging.getLogger('main')\n",
    "logging.basicConfig(\n",
    "    format='[%(asctime)s] [%(levelname)s] - %(message)s',\n",
    "    datefmt='%H:%M:%S'\n",
    ")\n",
    "\n",
    "# we can change the logging level. Use logging.DEBUG if necessary\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "def scrape_an_object(location):\n",
    "    # setting the chrome driver for selenium\n",
    "    driver = webdriver.Chrome(service=Service(DRIVER_PATH))\n",
    "\n",
    "    # opening the given URL\n",
    "    logger.debug(\"Opening the given URL\")\n",
    "    driver.get(OBJECT_URLS)\n",
    "\n",
    "    # accepting the cookies\n",
    "    logger.debug(\"Accepting the cookies\")\n",
    "    driver.find_element(By.CLASS_NAME, \"lssxud\").click()\n",
    "\n",
    "    # waiting some random seconds\n",
    "    time.sleep(random.uniform(4, 6))\n",
    "    select_box = driver.find_element(By.XPATH, '//*[@id=\"searchboxinput\"]')\n",
    "    select_box.send_keys(location)\n",
    "    select_box.send_keys(Keys.ENTER)\n",
    "    time.sleep(2)\n",
    "    object_name = driver.find_element(\n",
    "        By.CSS_SELECTOR,\n",
    "        'h1.DUwDvf.fontHeadlineLarge'\n",
    "    ).text\n",
    "    logger.debug(f'Object_name OK : {object_name}')\n",
    "\n",
    "    object_address = driver.find_element(\n",
    "        By.CSS_SELECTOR,\n",
    "        'div.Io6YTe.fontBodyMedium'\n",
    "    ).text\n",
    "    logger.debug(f'Object_address OK : {object_address}')\n",
    "\n",
    "    # I use CSS selectors where I can because it's more robust than XPATH\n",
    "\n",
    "    try:\n",
    "        overall_rating = driver.find_element(\n",
    "            By.CSS_SELECTOR,\n",
    "            'div.F7nice.mmu3tf'\n",
    "        ).text.split()[0]\n",
    "        logger.debug(f'Overall_rating OK : {overall_rating}')\n",
    "\n",
    "        review_number = driver.find_element(\n",
    "            By.CSS_SELECTOR,\n",
    "            'div.F7nice.mmu3tf'\n",
    "        ).text.replace(' ', '')\n",
    "\n",
    "        review_number = int(re.compile(r'\\d+').findall(review_number)[-1])\n",
    "        logger.debug(f'Review_number OK : {review_number}')\n",
    "\n",
    "        # click to load further reviews\n",
    "        driver.find_element(\n",
    "            By.XPATH,\n",
    "            '//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[2]/div[1]/div[1]/div[2]/div/div[1]/div[2]/span[2]/span[1]/span'\n",
    "        ).click()\n",
    "        try:\n",
    "            driver.find_element(\n",
    "                By.CSS_SELECTOR,\n",
    "                'button.Aq14fc'\n",
    "            ).click()\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "\n",
    "        logger.debug('Clicked to load further reviews')\n",
    "\n",
    "        time.sleep(random.uniform(0.1, 0.5))\n",
    "\n",
    "        # find scroll layout\n",
    "        scrollable_div = driver.find_element(\n",
    "            By.XPATH,\n",
    "            '//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[2]'\n",
    "        )\n",
    "\n",
    "        logger.debug('Scroll div OK')\n",
    "\n",
    "    except NoSuchElementException:\n",
    "\n",
    "        logger.debug('Except branch')\n",
    "\n",
    "        div_num_rating = driver.find_element(\n",
    "            By.CSS_SELECTOR,\n",
    "            'div.F7nice'\n",
    "        ).text\n",
    "        overall_rating = div_num_rating.split()[0]\n",
    "        logger.debug(f'Overall_rating OK : {overall_rating}')\n",
    "\n",
    "        review_number = int(div_num_rating.split()[1].replace('(', '').replace(')', ''))\n",
    "        logger.debug(f'Review_number OK : {review_number}')\n",
    "\n",
    "        # click on the review tab\n",
    "        driver.find_element(By.XPATH, '/html/body/div[3]/div[9]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[3]/div[1]').click()\n",
    "        logger.debug('clicked to load further reviews')\n",
    "\n",
    "        time.sleep(random.uniform(0.1, 0.5))\n",
    "\n",
    "        # find scroll layout\n",
    "        scrollable_div = driver.find_element(\n",
    "            By.XPATH,\n",
    "            '//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[3]'\n",
    "        )\n",
    "        logger.debug('Scroll div OK')\n",
    "\n",
    "    time.sleep(random.uniform(2, 4))\n",
    "\n",
    "    # scroll as many times as necessary to load all reviews\n",
    "    while True:\n",
    "        # Get the current height of the scrollable div\n",
    "        current_height = driver.execute_script('return arguments[0].scrollHeight', scrollable_div)\n",
    "\n",
    "        # Scroll to the bottom of the scrollable div\n",
    "        driver.execute_script('arguments[0].scrollTop = arguments[0].scrollHeight', scrollable_div)\n",
    "\n",
    "        # Wait for some time to load more reviews\n",
    "        time.sleep(random.uniform(1, 2))\n",
    "\n",
    "        # Get the new height of the scrollable div after scrolling\n",
    "        new_height = driver.execute_script('return arguments[0].scrollHeight', scrollable_div)\n",
    "\n",
    "        # click on 'more' button if it appears\n",
    "        try:\n",
    "            driver.find_element(\n",
    "                By.CSS_SELECTOR,\n",
    "                'button.w8nwRe.kyuRq'\n",
    "            ).click()\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "\n",
    "        # Check if the scrollable div height has changed\n",
    "        if new_height == current_height:\n",
    "            # No more reviews to load, exit the loop\n",
    "            break\n",
    "\n",
    "    # parse the HTML with a BeautifulSoup object\n",
    "    response = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    reviews_source = response.find_all('div', class_='jJc9Ad')\n",
    "    logger.debug('Source code has been parsed!')\n",
    "\n",
    "    # closing the browser\n",
    "    driver.quit()\n",
    "\n",
    "    # storing the data in a dict\n",
    "    store_main_data = {'object_name': object_name,\n",
    "                       'object_address': object_address,\n",
    "                       'overall_rating': overall_rating,\n",
    "                       'review_num': review_number,\n",
    "                       'object_url': OBJECT_URLS}\n",
    "\n",
    "    return store_main_data, reviews_source\n",
    "\n",
    "\n",
    "def extract_reviews(reviews_source: list) -> list:\n",
    "    \"\"\"\n",
    "    This method processes the input HTML code and returns a list containing the reviews.\n",
    "    \"\"\"\n",
    "    review_list = []\n",
    "\n",
    "    logger.debug('Starting to iterate through the reviews...')\n",
    "    for review in reviews_source:\n",
    "        # Extract the relevant information\n",
    "        user = review.find('div', class_='d4r55').text.strip()\n",
    "        date = review.find('span', class_='rsqaWe').text.strip()\n",
    "        # Find rating elements and extract the ratings\n",
    "        rate_elements = review.find_all('span', class_='kvMYJc')\n",
    "        rate = int(rate_elements[0].get('aria-label').split()[0])\n",
    "\n",
    "        review_text = review.find('span', class_='wiI7pd')\n",
    "        review_text = '' if review_text is None else review_text.text\n",
    "\n",
    "        review_list.append({\n",
    "            'name': user,\n",
    "            'date': date,\n",
    "            'rate': rate,\n",
    "            'review_text': review_text\n",
    "        })\n",
    "\n",
    "    return review_list\n",
    "\n",
    "\n",
    "def main():\n",
    "    # creating a list to store all objects scraped\n",
    "    all_objects_data = []\n",
    "\n",
    "    # iterating through each object URL in the list\n",
    "    for location in df_pe['location']:\n",
    "        # Get the corresponding object URL\n",
    "        \n",
    "        # Scrape data for the current location\n",
    "        store_main_data, reviews_source = scrape_an_object(location)\n",
    "\n",
    "        review_list = extract_reviews(reviews_source)\n",
    "        for review in review_list:\n",
    "            review.update(store_main_data)  # Add the common data to each review\n",
    "        all_objects_data.extend(review_list)\n",
    "        logger.debug(f'{location} is done!')\n",
    "\n",
    "        # Convert the scraped data to a DataFrame\n",
    "        df = pd.DataFrame(all_objects_data)\n",
    "\n",
    "        # Rearrange the columns\n",
    "        df = df[['name', 'date', 'rate', 'review_text', 'object_name', 'object_address', 'overall_rating', 'review_num', 'object_url']]\n",
    "\n",
    "        # Create the file path for saving the CSV file\n",
    "        csv_file_location = os.path.join(SAVING_PATH, f'google_reviews_{location}.csv')\n",
    "\n",
    "        # Write the DataFrame to a CSV file\n",
    "        df.to_csv(csv_file_location, index=False)\n",
    "        logger.debug(f\"CSV file saved at: {csv_file_location}\")\n",
    "\n",
    "    # writing the complete dataframe to a single csv file\n",
    "    all_csv_file_location = os.path.join(SAVING_PATH, 'google_reviews.csv')\n",
    "    df.to_csv(all_csv_file_location, index=False)\n",
    "    logger.debug(f\"All objects CSV file saved at: {all_csv_file_location}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusion des fichiers CSV terminée.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Définir le répertoire contenant les fichiers CSV à fusionner\n",
    "directory = '/Users/camille/repo/Hetic/projet_gouv/scraping/Data'\n",
    "# Lister tous les fichiers CSV dans le répertoire\n",
    "csv_files = [file for file in os.listdir(directory) if file.endswith('.csv') and file.startswith('google_reviews')]\n",
    "\n",
    "# Créer une liste pour stocker les DataFrames de chaque fichier CSV\n",
    "dataframes = []\n",
    "\n",
    "# Parcourir tous les fichiers CSV et charger les données dans des DataFrames\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(directory, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Fusionner tous les DataFrames en un seul DataFrame\n",
    "merged_df = pd.concat(dataframes)\n",
    "\n",
    "# Définir le chemin et le nom de fichier pour le fichier CSV fusionné\n",
    "output_file =  '/Users/camille/repo/Hetic/projet_gouv/scraping/output/google_reviews.csv'\n",
    "\n",
    "# Enregistrer le DataFrame fusionné dans un fichier CSV\n",
    "merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(\"Fusion des fichiers CSV terminée.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusion des fichiers CSV terminée.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Définir le répertoire contenant les fichiers CSV à fusionner\n",
    "directory = '/Users/camille/repo/Hetic/projet_gouv/scraping/Data'\n",
    "# Lister tous les fichiers CSV dans le répertoire\n",
    "csv_files = [file for file in os.listdir(directory) if file.endswith('.csv') and file.startswith('google_reviews')]\n",
    "\n",
    "# Créer une liste pour stocker les DataFrames de chaque fichier CSV\n",
    "dataframes = []\n",
    "\n",
    "# Parcourir tous les fichiers CSV et charger les données dans des DataFrames\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(directory, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    dataframes.append(df)\n",
    "    # Supprimer le fichier utilisé pour la fusion\n",
    "    os.remove(file_path)\n",
    "\n",
    "# Fusionner tous les DataFrames en un seul DataFrame\n",
    "merged_df = pd.concat(dataframes)\n",
    "\n",
    "# Définir le chemin et le nom de fichier pour le fichier CSV fusionné\n",
    "output_file =  '/Users/camille/repo/Hetic/projet_gouv/scraping/output/google_reviews.csv'\n",
    "\n",
    "# Enregistrer le DataFrame fusionné dans un fichier CSV\n",
    "merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(\"Fusion des fichiers CSV terminée.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('/Users/camille/repo/Hetic/projet_gouv/scraping/output/google_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>date</th>\n",
       "      <th>rate</th>\n",
       "      <th>review_text</th>\n",
       "      <th>object_name</th>\n",
       "      <th>object_address</th>\n",
       "      <th>overall_rating</th>\n",
       "      <th>review_num</th>\n",
       "      <th>object_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anna Pronina</td>\n",
       "      <td>6 years ago</td>\n",
       "      <td>5</td>\n",
       "      <td>Perfect service! The team is so professional. ...</td>\n",
       "      <td>Pôle emploi</td>\n",
       "      <td>10 Rue Brancion, 75015 Paris</td>\n",
       "      <td>2.8</td>\n",
       "      <td>62</td>\n",
       "      <td>https://www.google.com/maps/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alexandre NGUYEN VAN TY</td>\n",
       "      <td>2 months ago</td>\n",
       "      <td>1</td>\n",
       "      <td>Inadmissible et honteux tout simplement. Atten...</td>\n",
       "      <td>Pôle emploi</td>\n",
       "      <td>10 Rue Brancion, 75015 Paris</td>\n",
       "      <td>2.8</td>\n",
       "      <td>62</td>\n",
       "      <td>https://www.google.com/maps/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Down Me</td>\n",
       "      <td>4 months ago</td>\n",
       "      <td>2</td>\n",
       "      <td>Accueilli par une personne féminine d'un âge c...</td>\n",
       "      <td>Pôle emploi</td>\n",
       "      <td>10 Rue Brancion, 75015 Paris</td>\n",
       "      <td>2.8</td>\n",
       "      <td>62</td>\n",
       "      <td>https://www.google.com/maps/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hassan Bashir</td>\n",
       "      <td>2 weeks ago</td>\n",
       "      <td>1</td>\n",
       "      <td>Je suis Hassan Bashir, j'ai besoin de rendez-v...</td>\n",
       "      <td>Pôle emploi</td>\n",
       "      <td>10 Rue Brancion, 75015 Paris</td>\n",
       "      <td>2.8</td>\n",
       "      <td>62</td>\n",
       "      <td>https://www.google.com/maps/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Caroline Av</td>\n",
       "      <td>a year ago</td>\n",
       "      <td>1</td>\n",
       "      <td>De loin la pire agence.\\nD'une incompétence sa...</td>\n",
       "      <td>Pôle emploi</td>\n",
       "      <td>10 Rue Brancion, 75015 Paris</td>\n",
       "      <td>2.8</td>\n",
       "      <td>62</td>\n",
       "      <td>https://www.google.com/maps/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2200</th>\n",
       "      <td>Mohammed Amir</td>\n",
       "      <td>6 years ago</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pôle emploi</td>\n",
       "      <td>9 Rue Friant, 75014 Paris</td>\n",
       "      <td>2.8</td>\n",
       "      <td>40</td>\n",
       "      <td>https://www.google.com/maps/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2201</th>\n",
       "      <td>Renato De Sousa</td>\n",
       "      <td>4 years ago</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pôle emploi</td>\n",
       "      <td>9 Rue Friant, 75014 Paris</td>\n",
       "      <td>2.8</td>\n",
       "      <td>40</td>\n",
       "      <td>https://www.google.com/maps/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2202</th>\n",
       "      <td>nadia yahiaoui</td>\n",
       "      <td>5 years ago</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pôle emploi</td>\n",
       "      <td>9 Rue Friant, 75014 Paris</td>\n",
       "      <td>2.8</td>\n",
       "      <td>40</td>\n",
       "      <td>https://www.google.com/maps/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2203</th>\n",
       "      <td>Roger Gonzalez</td>\n",
       "      <td>7 months ago</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pôle emploi</td>\n",
       "      <td>9 Rue Friant, 75014 Paris</td>\n",
       "      <td>2.8</td>\n",
       "      <td>40</td>\n",
       "      <td>https://www.google.com/maps/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2204</th>\n",
       "      <td>Eric Genier</td>\n",
       "      <td>5 years ago</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pôle emploi</td>\n",
       "      <td>9 Rue Friant, 75014 Paris</td>\n",
       "      <td>2.8</td>\n",
       "      <td>40</td>\n",
       "      <td>https://www.google.com/maps/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>616 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         name          date  rate   \n",
       "0                Anna Pronina   6 years ago     5  \\\n",
       "1     Alexandre NGUYEN VAN TY  2 months ago     1   \n",
       "2                     Down Me  4 months ago     2   \n",
       "3               Hassan Bashir   2 weeks ago     1   \n",
       "4                 Caroline Av    a year ago     1   \n",
       "...                       ...           ...   ...   \n",
       "2200            Mohammed Amir   6 years ago     5   \n",
       "2201          Renato De Sousa   4 years ago     4   \n",
       "2202           nadia yahiaoui   5 years ago     1   \n",
       "2203           Roger Gonzalez  7 months ago     1   \n",
       "2204              Eric Genier   5 years ago     2   \n",
       "\n",
       "                                            review_text  object_name   \n",
       "0     Perfect service! The team is so professional. ...  Pôle emploi  \\\n",
       "1     Inadmissible et honteux tout simplement. Atten...  Pôle emploi   \n",
       "2     Accueilli par une personne féminine d'un âge c...  Pôle emploi   \n",
       "3     Je suis Hassan Bashir, j'ai besoin de rendez-v...  Pôle emploi   \n",
       "4     De loin la pire agence.\\nD'une incompétence sa...  Pôle emploi   \n",
       "...                                                 ...          ...   \n",
       "2200                                                NaN  Pôle emploi   \n",
       "2201                                                NaN  Pôle emploi   \n",
       "2202                                                NaN  Pôle emploi   \n",
       "2203                                                NaN  Pôle emploi   \n",
       "2204                                                NaN  Pôle emploi   \n",
       "\n",
       "                    object_address  overall_rating  review_num   \n",
       "0     10 Rue Brancion, 75015 Paris             2.8          62  \\\n",
       "1     10 Rue Brancion, 75015 Paris             2.8          62   \n",
       "2     10 Rue Brancion, 75015 Paris             2.8          62   \n",
       "3     10 Rue Brancion, 75015 Paris             2.8          62   \n",
       "4     10 Rue Brancion, 75015 Paris             2.8          62   \n",
       "...                            ...             ...         ...   \n",
       "2200     9 Rue Friant, 75014 Paris             2.8          40   \n",
       "2201     9 Rue Friant, 75014 Paris             2.8          40   \n",
       "2202     9 Rue Friant, 75014 Paris             2.8          40   \n",
       "2203     9 Rue Friant, 75014 Paris             2.8          40   \n",
       "2204     9 Rue Friant, 75014 Paris             2.8          40   \n",
       "\n",
       "                        object_url  \n",
       "0     https://www.google.com/maps/  \n",
       "1     https://www.google.com/maps/  \n",
       "2     https://www.google.com/maps/  \n",
       "3     https://www.google.com/maps/  \n",
       "4     https://www.google.com/maps/  \n",
       "...                            ...  \n",
       "2200  https://www.google.com/maps/  \n",
       "2201  https://www.google.com/maps/  \n",
       "2202  https://www.google.com/maps/  \n",
       "2203  https://www.google.com/maps/  \n",
       "2204  https://www.google.com/maps/  \n",
       "\n",
       "[616 rows x 9 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gouv_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
