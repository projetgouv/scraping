{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code scraping fonctionnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_pe = pd.read_csv('/Users/camille/repo/Hetic/projet_gouv/scraping/Data/location.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[13:51:09] [DEBUG] - Opening the given URL\n",
      "[13:51:10] [DEBUG] - Accepting the cookies\n",
      "[13:51:17] [DEBUG] - Object_name OK : Pôle emploi\n",
      "[13:51:17] [DEBUG] - Object_address OK : 10 Rue Brancion, 75015 Paris\n",
      "[13:51:17] [DEBUG] - Except branch\n",
      "[13:51:17] [DEBUG] - Overall_rating OK : 2.8\n",
      "[13:51:17] [DEBUG] - Review_number OK : 62\n",
      "[13:51:17] [DEBUG] - clicked to load further reviews\n",
      "[13:51:18] [DEBUG] - Scroll div OK\n",
      "[13:51:33] [DEBUG] - Source code has been parsed!\n",
      "[13:51:33] [DEBUG] - Starting to iterate through the reviews...\n",
      "[13:51:33] [DEBUG] - pole emploi AVS Placement Artistes 10 RUE BRANCION 75015 Paris is done!\n",
      "[13:51:33] [DEBUG] - CSV file saved at: /Users/camille/repo/Hetic/projet_gouv/scraping/Data/google_reviews_pole emploi AVS Placement Artistes 10 RUE BRANCION 75015 Paris.csv\n",
      "[13:51:35] [DEBUG] - Opening the given URL\n",
      "[13:51:35] [DEBUG] - Accepting the cookies\n",
      "[13:51:43] [DEBUG] - Object_name OK : Pôle Emploi\n",
      "[13:51:43] [DEBUG] - Object_address OK : 11 Rue Pelée, 75011 Paris\n",
      "[13:51:43] [DEBUG] - Except branch\n",
      "[13:51:43] [DEBUG] - Overall_rating OK : 2.6\n",
      "[13:51:43] [DEBUG] - Review_number OK : 32\n",
      "[13:51:43] [DEBUG] - clicked to load further reviews\n",
      "[13:51:43] [DEBUG] - Scroll div OK\n",
      "[13:51:52] [DEBUG] - Source code has been parsed!\n",
      "[13:51:52] [DEBUG] - Starting to iterate through the reviews...\n",
      "[13:51:52] [DEBUG] - Pole emploi Beaumarchais 11 RUE PELEE 75011 Paris is done!\n",
      "[13:51:52] [DEBUG] - CSV file saved at: /Users/camille/repo/Hetic/projet_gouv/scraping/Data/google_reviews_Pole emploi Beaumarchais 11 RUE PELEE 75011 Paris.csv\n",
      "[13:51:54] [DEBUG] - Opening the given URL\n",
      "[13:51:54] [DEBUG] - Accepting the cookies\n",
      "[13:52:02] [DEBUG] - Object_name OK : Pôle Emploi - Cardinet\n",
      "[13:52:02] [DEBUG] - Object_address OK : 8 Rue Bernard Buffet, 75017 Paris\n",
      "[13:52:02] [DEBUG] - Except branch\n",
      "[13:52:02] [DEBUG] - Overall_rating OK : 2.4\n",
      "[13:52:02] [DEBUG] - Review_number OK : 77\n",
      "[13:52:03] [DEBUG] - clicked to load further reviews\n",
      "[13:52:03] [DEBUG] - Scroll div OK\n",
      "[13:52:18] [DEBUG] - Source code has been parsed!\n",
      "[13:52:18] [DEBUG] - Starting to iterate through the reviews...\n",
      "[13:52:18] [DEBUG] - Pole emploi Cardinet 8 RUE Bernard Buffet 75017 Paris is done!\n",
      "[13:52:18] [DEBUG] - CSV file saved at: /Users/camille/repo/Hetic/projet_gouv/scraping/Data/google_reviews_Pole emploi Cardinet 8 RUE Bernard Buffet 75017 Paris.csv\n",
      "[13:52:20] [DEBUG] - Opening the given URL\n",
      "[13:52:20] [DEBUG] - Accepting the cookies\n",
      "[13:52:28] [DEBUG] - Object_name OK : Pôle emploi\n",
      "[13:52:28] [DEBUG] - Object_address OK : 44 Rue Armand Carrel, 75019 Paris\n",
      "[13:52:28] [DEBUG] - Except branch\n",
      "[13:52:28] [DEBUG] - Overall_rating OK : 2.4\n",
      "[13:52:28] [DEBUG] - Review_number OK : 47\n",
      "[13:52:28] [DEBUG] - clicked to load further reviews\n",
      "[13:52:28] [DEBUG] - Scroll div OK\n",
      "[13:52:39] [DEBUG] - Source code has been parsed!\n",
      "[13:52:39] [DEBUG] - Starting to iterate through the reviews...\n",
      "[13:52:39] [DEBUG] - pole emploi Armand Carrel 44 RUE Armand Carrel 75019 Paris is done!\n",
      "[13:52:39] [DEBUG] - CSV file saved at: /Users/camille/repo/Hetic/projet_gouv/scraping/Data/google_reviews_pole emploi Armand Carrel 44 RUE Armand Carrel 75019 Paris.csv\n",
      "[13:52:41] [DEBUG] - Opening the given URL\n",
      "[13:52:41] [DEBUG] - Accepting the cookies\n",
      "[13:52:48] [DEBUG] - Object_name OK : Pôle emploi\n",
      "[13:52:48] [DEBUG] - Object_address OK : 60 Rue Vitruve, 75020 Paris\n",
      "[13:52:48] [DEBUG] - Except branch\n",
      "[13:52:48] [DEBUG] - Overall_rating OK : 2.9\n",
      "[13:52:48] [DEBUG] - Review_number OK : 35\n",
      "[13:52:48] [DEBUG] - clicked to load further reviews\n",
      "[13:52:48] [DEBUG] - Scroll div OK\n",
      "[13:53:00] [DEBUG] - Source code has been parsed!\n",
      "[13:53:00] [DEBUG] - Starting to iterate through the reviews...\n",
      "[13:53:00] [DEBUG] - Pole emploi Vitruve 60 RUE Vitruve 75020 Paris is done!\n",
      "[13:53:00] [DEBUG] - CSV file saved at: /Users/camille/repo/Hetic/projet_gouv/scraping/Data/google_reviews_Pole emploi Vitruve 60 RUE Vitruve 75020 Paris.csv\n",
      "[13:53:01] [DEBUG] - Opening the given URL\n",
      "[13:53:02] [DEBUG] - Accepting the cookies\n",
      "[13:53:08] [DEBUG] - Object_name OK : Pôle emploi\n",
      "[13:53:08] [DEBUG] - Object_address OK : 27 Rue Daviel, 75013 Paris\n",
      "[13:53:08] [DEBUG] - Except branch\n",
      "[13:53:08] [DEBUG] - Overall_rating OK : 2.5\n",
      "[13:53:08] [DEBUG] - Review_number OK : 82\n",
      "[13:53:08] [DEBUG] - clicked to load further reviews\n",
      "[13:53:09] [DEBUG] - Scroll div OK\n",
      "[13:53:28] [DEBUG] - Source code has been parsed!\n",
      "[13:53:29] [DEBUG] - Starting to iterate through the reviews...\n",
      "[13:53:29] [DEBUG] - Pole emploi Daviel 27 RUE Daviel 75013 Paris is done!\n",
      "[13:53:29] [DEBUG] - CSV file saved at: /Users/camille/repo/Hetic/projet_gouv/scraping/Data/google_reviews_Pole emploi Daviel 27 RUE Daviel 75013 Paris.csv\n",
      "[13:53:30] [DEBUG] - Opening the given URL\n",
      "[13:53:30] [DEBUG] - Accepting the cookies\n",
      "[13:53:38] [DEBUG] - Object_name OK : Pôle emploi\n",
      "[13:53:38] [DEBUG] - Object_address OK : 75 Av. Jean Jaurès, 75019 Paris\n",
      "[13:53:38] [DEBUG] - Except branch\n",
      "[13:53:38] [DEBUG] - Overall_rating OK : 3.2\n",
      "[13:53:38] [DEBUG] - Review_number OK : 25\n",
      "[13:53:38] [DEBUG] - clicked to load further reviews\n",
      "[13:53:38] [DEBUG] - Scroll div OK\n",
      "[13:53:46] [DEBUG] - Source code has been parsed!\n",
      "[13:53:46] [DEBUG] - Starting to iterate through the reviews...\n",
      "[13:53:46] [DEBUG] - Pole emploi Laumière 75 AVENUE JEAN JAURES 75019 Paris is done!\n",
      "[13:53:46] [DEBUG] - CSV file saved at: /Users/camille/repo/Hetic/projet_gouv/scraping/Data/google_reviews_Pole emploi Laumière 75 AVENUE JEAN JAURES 75019 Paris.csv\n",
      "[13:53:48] [DEBUG] - Opening the given URL\n",
      "[13:53:48] [DEBUG] - Accepting the cookies\n",
      "[13:53:56] [DEBUG] - Object_name OK : Pôle emploi\n",
      "[13:53:56] [DEBUG] - Object_address OK : 11 Rue Maurice Genevoix, 75018 Paris\n",
      "[13:53:56] [DEBUG] - Except branch\n",
      "[13:53:56] [DEBUG] - Overall_rating OK : 3.0\n",
      "[13:53:56] [DEBUG] - Review_number OK : 29\n",
      "[13:53:56] [DEBUG] - clicked to load further reviews\n",
      "[13:53:56] [DEBUG] - Scroll div OK\n",
      "[13:54:04] [DEBUG] - Source code has been parsed!\n",
      "[13:54:04] [DEBUG] - Starting to iterate through the reviews...\n",
      "[13:54:04] [DEBUG] - Pole emploi Genevoix 11 RUE Maurice Genevoix 75018 Paris is done!\n",
      "[13:54:04] [DEBUG] - CSV file saved at: /Users/camille/repo/Hetic/projet_gouv/scraping/Data/google_reviews_Pole emploi Genevoix 11 RUE Maurice Genevoix 75018 Paris.csv\n",
      "[13:54:06] [DEBUG] - Opening the given URL\n",
      "[13:54:06] [DEBUG] - Accepting the cookies\n",
      "[13:54:13] [DEBUG] - Object_name OK : Pôle emploi\n",
      "[13:54:13] [DEBUG] - Object_address OK : 3 Bd Diderot, 75012 Paris\n",
      "[13:54:13] [DEBUG] - Except branch\n",
      "[13:54:13] [DEBUG] - Overall_rating OK : 2.1\n",
      "[13:54:13] [DEBUG] - Review_number OK : 54\n",
      "[13:54:13] [DEBUG] - clicked to load further reviews\n",
      "[13:54:14] [DEBUG] - Scroll div OK\n",
      "[13:54:28] [DEBUG] - Source code has been parsed!\n",
      "[13:54:28] [DEBUG] - Starting to iterate through the reviews...\n",
      "[13:54:28] [DEBUG] - Pole emploi Diderot 3 BOULEVARD Diderot 75012 Paris is done!\n",
      "[13:54:28] [DEBUG] - CSV file saved at: /Users/camille/repo/Hetic/projet_gouv/scraping/Data/google_reviews_Pole emploi Diderot 3 BOULEVARD Diderot 75012 Paris.csv\n",
      "[13:54:29] [DEBUG] - Opening the given URL\n",
      "[13:54:30] [DEBUG] - Accepting the cookies\n"
     ]
    },
    {
     "ename": "NoSuchElementException",
     "evalue": "Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"h1.DUwDvf.fontHeadlineLarge\"}\n  (Session info: chrome=113.0.5672.126)\nStacktrace:\n0   chromedriver                        0x000000010064f8ac chromedriver + 4257964\n1   chromedriver                        0x0000000100647f40 chromedriver + 4226880\n2   chromedriver                        0x00000001002849d4 chromedriver + 281044\n3   chromedriver                        0x00000001002bfa34 chromedriver + 522804\n4   chromedriver                        0x00000001002f67e4 chromedriver + 747492\n5   chromedriver                        0x00000001002b398c chromedriver + 473484\n6   chromedriver                        0x00000001002b498c chromedriver + 477580\n7   chromedriver                        0x000000010060e900 chromedriver + 3991808\n8   chromedriver                        0x0000000100612354 chromedriver + 4006740\n9   chromedriver                        0x0000000100612940 chromedriver + 4008256\n10  chromedriver                        0x000000010061833c chromedriver + 4031292\n11  chromedriver                        0x0000000100612f34 chromedriver + 4009780\n12  chromedriver                        0x00000001005eb490 chromedriver + 3847312\n13  chromedriver                        0x00000001006309f4 chromedriver + 4131316\n14  chromedriver                        0x0000000100630b4c chromedriver + 4131660\n15  chromedriver                        0x0000000100641230 chromedriver + 4198960\n16  libsystem_pthread.dylib             0x0000000196dea06c _pthread_start + 148\n17  libsystem_pthread.dylib             0x0000000196de4e2c thread_start + 8\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 251\u001b[0m\n\u001b[1;32m    248\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAll objects CSV file saved at: \u001b[39m\u001b[39m{\u001b[39;00mall_csv_file_location\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    250\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 251\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[52], line 224\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[39m# iterating through each object URL in the list\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[39mfor\u001b[39;00m location \u001b[39min\u001b[39;00m df_pe[\u001b[39m'\u001b[39m\u001b[39mlocation\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m    221\u001b[0m     \u001b[39m# Get the corresponding object URL\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \n\u001b[1;32m    223\u001b[0m     \u001b[39m# Scrape data for the current location\u001b[39;00m\n\u001b[0;32m--> 224\u001b[0m     store_main_data, reviews_source \u001b[39m=\u001b[39m scrape_an_object(location)\n\u001b[1;32m    226\u001b[0m     review_list \u001b[39m=\u001b[39m extract_reviews(reviews_source)\n\u001b[1;32m    227\u001b[0m     \u001b[39mfor\u001b[39;00m review \u001b[39min\u001b[39;00m review_list:\n",
      "Cell \u001b[0;32mIn[52], line 52\u001b[0m, in \u001b[0;36mscrape_an_object\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m     50\u001b[0m select_box\u001b[39m.\u001b[39msend_keys(Keys\u001b[39m.\u001b[39mENTER)\n\u001b[1;32m     51\u001b[0m time\u001b[39m.\u001b[39msleep(\u001b[39m2\u001b[39m)\n\u001b[0;32m---> 52\u001b[0m object_name \u001b[39m=\u001b[39m driver\u001b[39m.\u001b[39;49mfind_element(\n\u001b[1;32m     53\u001b[0m     By\u001b[39m.\u001b[39;49mCSS_SELECTOR,\n\u001b[1;32m     54\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mh1.DUwDvf.fontHeadlineLarge\u001b[39;49m\u001b[39m'\u001b[39;49m\n\u001b[1;32m     55\u001b[0m )\u001b[39m.\u001b[39mtext\n\u001b[1;32m     56\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mObject_name OK : \u001b[39m\u001b[39m{\u001b[39;00mobject_name\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     58\u001b[0m object_address \u001b[39m=\u001b[39m driver\u001b[39m.\u001b[39mfind_element(\n\u001b[1;32m     59\u001b[0m     By\u001b[39m.\u001b[39mCSS_SELECTOR,\n\u001b[1;32m     60\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdiv.Io6YTe.fontBodyMedium\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     61\u001b[0m )\u001b[39m.\u001b[39mtext\n",
      "File \u001b[0;32m~/repo/Hetic/projet_gouv/scraping/clean_data_env/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py:831\u001b[0m, in \u001b[0;36mWebDriver.find_element\u001b[0;34m(self, by, value)\u001b[0m\n\u001b[1;32m    828\u001b[0m     by \u001b[39m=\u001b[39m By\u001b[39m.\u001b[39mCSS_SELECTOR\n\u001b[1;32m    829\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m[name=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 831\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(Command\u001b[39m.\u001b[39;49mFIND_ELEMENT, {\u001b[39m\"\u001b[39;49m\u001b[39musing\u001b[39;49m\u001b[39m\"\u001b[39;49m: by, \u001b[39m\"\u001b[39;49m\u001b[39mvalue\u001b[39;49m\u001b[39m\"\u001b[39;49m: value})[\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/repo/Hetic/projet_gouv/scraping/clean_data_env/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py:440\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    438\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_executor\u001b[39m.\u001b[39mexecute(driver_command, params)\n\u001b[1;32m    439\u001b[0m \u001b[39mif\u001b[39;00m response:\n\u001b[0;32m--> 440\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror_handler\u001b[39m.\u001b[39;49mcheck_response(response)\n\u001b[1;32m    441\u001b[0m     response[\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unwrap_value(response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    442\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/repo/Hetic/projet_gouv/scraping/clean_data_env/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py:245\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    243\u001b[0m         alert_text \u001b[39m=\u001b[39m value[\u001b[39m\"\u001b[39m\u001b[39malert\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    244\u001b[0m     \u001b[39mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[39m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[0;32m--> 245\u001b[0m \u001b[39mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[0;31mNoSuchElementException\u001b[0m: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"h1.DUwDvf.fontHeadlineLarge\"}\n  (Session info: chrome=113.0.5672.126)\nStacktrace:\n0   chromedriver                        0x000000010064f8ac chromedriver + 4257964\n1   chromedriver                        0x0000000100647f40 chromedriver + 4226880\n2   chromedriver                        0x00000001002849d4 chromedriver + 281044\n3   chromedriver                        0x00000001002bfa34 chromedriver + 522804\n4   chromedriver                        0x00000001002f67e4 chromedriver + 747492\n5   chromedriver                        0x00000001002b398c chromedriver + 473484\n6   chromedriver                        0x00000001002b498c chromedriver + 477580\n7   chromedriver                        0x000000010060e900 chromedriver + 3991808\n8   chromedriver                        0x0000000100612354 chromedriver + 4006740\n9   chromedriver                        0x0000000100612940 chromedriver + 4008256\n10  chromedriver                        0x000000010061833c chromedriver + 4031292\n11  chromedriver                        0x0000000100612f34 chromedriver + 4009780\n12  chromedriver                        0x00000001005eb490 chromedriver + 3847312\n13  chromedriver                        0x00000001006309f4 chromedriver + 4131316\n14  chromedriver                        0x0000000100630b4c chromedriver + 4131660\n15  chromedriver                        0x0000000100641230 chromedriver + 4198960\n16  libsystem_pthread.dylib             0x0000000196dea06c _pthread_start + 148\n17  libsystem_pthread.dylib             0x0000000196de4e2c thread_start + 8\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "DRIVER_PATH = r'/home/oli/Projects/Google-review-scraper/chromedriver_linux64/chromedriver'\n",
    "SAVING_PATH = '/Users/camille/repo/Hetic/projet_gouv/scraping/Data'\n",
    "\n",
    "# declaring a list, that contains the urls which we want to be scraped\n",
    "OBJECT_URLS = \"https://www.google.com/maps/\"\n",
    "df_pe = pd.read_csv('/Users/camille/repo/Hetic/projet_gouv/scraping/Data/location.csv', sep=';') \n",
    "\n",
    "# setting up the logging object\n",
    "logger = logging.getLogger('main')\n",
    "logging.basicConfig(\n",
    "    format='[%(asctime)s] [%(levelname)s] - %(message)s',\n",
    "    datefmt='%H:%M:%S'\n",
    ")\n",
    "\n",
    "# we can change the logging level. Use logging.DEBUG if necessary\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "def scrape_an_object(location):\n",
    "    # setting the chrome driver for selenium\n",
    "    driver = webdriver.Chrome(service=Service(DRIVER_PATH))\n",
    "\n",
    "    # opening the given URL\n",
    "    logger.debug(\"Opening the given URL\")\n",
    "    driver.get(OBJECT_URLS)\n",
    "\n",
    "    # accepting the cookies\n",
    "    logger.debug(\"Accepting the cookies\")\n",
    "    driver.find_element(By.CLASS_NAME, \"lssxud\").click()\n",
    "\n",
    "    # waiting some random seconds\n",
    "    time.sleep(random.uniform(4, 6))\n",
    "    select_box = driver.find_element(By.XPATH, '//*[@id=\"searchboxinput\"]')\n",
    "    select_box.send_keys(location)\n",
    "    select_box.send_keys(Keys.ENTER)\n",
    "    time.sleep(2)\n",
    "    object_name = driver.find_element(\n",
    "        By.CSS_SELECTOR,\n",
    "        'h1.DUwDvf.fontHeadlineLarge'\n",
    "    ).text\n",
    "    logger.debug(f'Object_name OK : {object_name}')\n",
    "\n",
    "    object_address = driver.find_element(\n",
    "        By.CSS_SELECTOR,\n",
    "        'div.Io6YTe.fontBodyMedium'\n",
    "    ).text\n",
    "    logger.debug(f'Object_address OK : {object_address}')\n",
    "\n",
    "    # I use CSS selectors where I can because it's more robust than XPATH\n",
    "\n",
    "    try:\n",
    "        overall_rating = driver.find_element(\n",
    "            By.CSS_SELECTOR,\n",
    "            'div.F7nice.mmu3tf'\n",
    "        ).text\n",
    "        logger.debug(f'Overall_rating OK : {overall_rating}')\n",
    "\n",
    "        review_number = driver.find_element(\n",
    "            By.CSS_SELECTOR,\n",
    "            'div.F7nice.mmu3tf'\n",
    "        ).text.replace(' ', '')\n",
    "\n",
    "        review_number = int(re.compile(r'\\d+').findall(review_number)[-1])\n",
    "        logger.debug(f'Review_number OK : {review_number}')\n",
    "\n",
    "        # click to load further reviews\n",
    "        driver.find_element(\n",
    "            By.XPATH,\n",
    "            '//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[2]/div[1]/div[1]/div[2]/div/div[1]/div[2]/span[2]/span[1]/span'\n",
    "        ).click()\n",
    "        try:\n",
    "            driver.find_element(\n",
    "                By.CSS_SELECTOR,\n",
    "                'button.Aq14fc'\n",
    "            ).click()\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "\n",
    "        logger.debug('Clicked to load further reviews')\n",
    "\n",
    "        time.sleep(random.uniform(0.1, 0.5))\n",
    "\n",
    "        # find scroll layout\n",
    "        scrollable_div = driver.find_element(\n",
    "            By.XPATH,\n",
    "            '//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[2]'\n",
    "        )\n",
    "\n",
    "        logger.debug('Scroll div OK')\n",
    "\n",
    "    except NoSuchElementException:\n",
    "\n",
    "        logger.debug('Except branch')\n",
    "\n",
    "        div_num_rating = driver.find_element(\n",
    "            By.CSS_SELECTOR,\n",
    "            'div.F7nice'\n",
    "        ).text\n",
    "        overall_rating = div_num_rating.split()[0]\n",
    "        logger.debug(f'Overall_rating OK : {overall_rating}')\n",
    "\n",
    "        review_number = int(div_num_rating.split()[1].replace('(', '').replace(')', ''))\n",
    "        logger.debug(f'Review_number OK : {review_number}')\n",
    "\n",
    "        # click on the review tab\n",
    "        driver.find_element(By.XPATH, '/html/body/div[3]/div[9]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[3]/div[1]').click()\n",
    "        logger.debug('clicked to load further reviews')\n",
    "\n",
    "        time.sleep(random.uniform(0.1, 0.5))\n",
    "\n",
    "        # find scroll layout\n",
    "        scrollable_div = driver.find_element(\n",
    "            By.XPATH,\n",
    "            '//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[3]'\n",
    "        )\n",
    "        logger.debug('Scroll div OK')\n",
    "\n",
    "    time.sleep(random.uniform(2, 4))\n",
    "\n",
    "    # scroll as many times as necessary to load all reviews\n",
    "    while True:\n",
    "        # Get the current height of the scrollable div\n",
    "        current_height = driver.execute_script('return arguments[0].scrollHeight', scrollable_div)\n",
    "\n",
    "        # Scroll to the bottom of the scrollable div\n",
    "        driver.execute_script('arguments[0].scrollTop = arguments[0].scrollHeight', scrollable_div)\n",
    "        try:\n",
    "            driver.find_element(\n",
    "                By.CSS_SELECTOR,\n",
    "                'button.w8nwRe.kyuRq'\n",
    "            ).click()\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "        # Wait for some time to load more reviews\n",
    "        time.sleep(random.uniform(1, 2))\n",
    "\n",
    "        # Get the new height of the scrollable div after scrolling\n",
    "        new_height = driver.execute_script('return arguments[0].scrollHeight', scrollable_div)\n",
    "\n",
    "        # click on 'more' button if it appears\n",
    "        try:\n",
    "            driver.find_element(\n",
    "                By.CSS_SELECTOR,\n",
    "                'button.w8nwRe.kyuRq'\n",
    "            ).click()\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "\n",
    "        # Check if the scrollable div height has changed\n",
    "        if new_height == current_height:\n",
    "            # No more reviews to load, exit the loop\n",
    "            break\n",
    "\n",
    "    # parse the HTML with a BeautifulSoup object\n",
    "    response = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    reviews_source = response.find_all('div', class_='jJc9Ad')\n",
    "    logger.debug('Source code has been parsed!')\n",
    "\n",
    "    # closing the browser\n",
    "    driver.quit()\n",
    "\n",
    "    # storing the data in a dict\n",
    "    store_main_data = {'object_name': object_name,\n",
    "                       'object_address': object_address,\n",
    "                       'overall_rating': overall_rating,\n",
    "                       'review_num': review_number,\n",
    "                       'object_url': OBJECT_URLS}\n",
    "\n",
    "    return store_main_data, reviews_source\n",
    "\n",
    "\n",
    "def extract_reviews(reviews_source: list) -> list:\n",
    "    \"\"\"\n",
    "    This method processes the input HTML code and returns a list containing the reviews.\n",
    "    \"\"\"\n",
    "    review_list = []\n",
    "\n",
    "    logger.debug('Starting to iterate through the reviews...')\n",
    "    for review in reviews_source:\n",
    "        # Extract the relevant information\n",
    "        user = review.find('div', class_='d4r55').text.strip()\n",
    "        date = review.find('span', class_='rsqaWe').text.strip()\n",
    "        # Find rating elements and extract the ratings\n",
    "        rate_elements = review.find_all('span', class_='kvMYJc')\n",
    "        rate = int(rate_elements[0].get('aria-label').split()[0])\n",
    "\n",
    "        review_text = review.find('span', class_='wiI7pd')\n",
    "        review_text = '' if review_text is None else review_text.text\n",
    "\n",
    "        review_list.append({\n",
    "            'name': user,\n",
    "            'date': date,\n",
    "            'rate': rate,\n",
    "            'review_text': review_text\n",
    "        })\n",
    "\n",
    "    return review_list\n",
    "\n",
    "\n",
    "def main():\n",
    "    # creating a list to store all objects scraped\n",
    "    all_objects_data = []\n",
    "\n",
    "    # iterating through each object URL in the list\n",
    "    for location in df_pe['location']:\n",
    "        # Get the corresponding object URL\n",
    "        \n",
    "        # Scrape data for the current location\n",
    "        store_main_data, reviews_source = scrape_an_object(location)\n",
    "\n",
    "        review_list = extract_reviews(reviews_source)\n",
    "        for review in review_list:\n",
    "            review.update(store_main_data)  # Add the common data to each review\n",
    "        all_objects_data.extend(review_list)\n",
    "        logger.debug(f'{location} is done!')\n",
    "\n",
    "        # Convert the scraped data to a DataFrame\n",
    "        df = pd.DataFrame(all_objects_data)\n",
    "\n",
    "        # Rearrange the columns\n",
    "        df = df[['name', 'date', 'rate', 'review_text', 'object_name', 'object_address', 'overall_rating', 'review_num', 'object_url']]\n",
    "\n",
    "        # Create the file path for saving the CSV file\n",
    "        csv_file_location = os.path.join(SAVING_PATH, f'google_reviews_{location}.csv')\n",
    "\n",
    "        # Write the DataFrame to a CSV file\n",
    "        df.to_csv(csv_file_location, index=False)\n",
    "        logger.debug(f\"CSV file saved at: {csv_file_location}\")\n",
    "\n",
    "    # writing the complete dataframe to a single csv file\n",
    "    all_csv_file_location = os.path.join(SAVING_PATH, 'google_reviews.csv')\n",
    "    df.to_csv(all_csv_file_location, index=False)\n",
    "    logger.debug(f\"All objects CSV file saved at: {all_csv_file_location}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusion des fichiers CSV terminée.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Définir le répertoire contenant les fichiers CSV à fusionner\n",
    "directory = '/Users/camille/repo/Hetic/projet_gouv/scraping/Data'\n",
    "# Lister tous les fichiers CSV dans le répertoire\n",
    "csv_files = [file for file in os.listdir(directory) if file.endswith('.csv') and file.startswith('google_reviews')]\n",
    "\n",
    "# Créer une liste pour stocker les DataFrames de chaque fichier CSV\n",
    "dataframes = []\n",
    "\n",
    "# Parcourir tous les fichiers CSV et charger les données dans des DataFrames\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(directory, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Fusionner tous les DataFrames en un seul DataFrame\n",
    "merged_df = pd.concat(dataframes)\n",
    "\n",
    "# Définir le chemin et le nom de fichier pour le fichier CSV fusionné\n",
    "output_file =  '/Users/camille/repo/Hetic/projet_gouv/scraping/output/google_reviews.csv'\n",
    "\n",
    "# Enregistrer le DataFrame fusionné dans un fichier CSV\n",
    "merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(\"Fusion des fichiers CSV terminée.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Définir le répertoire contenant les fichiers CSV à fusionner\n",
    "directory = '/Users/camille/repo/Hetic/projet_gouv/scraping/Data'\n",
    "# Lister tous les fichiers CSV dans le répertoire\n",
    "csv_files = [file for file in os.listdir(directory) if file.endswith('.csv') and file.startswith('google_reviews')]\n",
    "\n",
    "# Créer une liste pour stocker les DataFrames de chaque fichier CSV\n",
    "dataframes = []\n",
    "\n",
    "# Parcourir tous les fichiers CSV et charger les données dans des DataFrames\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(directory, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    dataframes.append(df)\n",
    "    # Supprimer le fichier utilisé pour la fusion\n",
    "    os.remove(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('/Users/camille/repo/Hetic/projet_gouv/scraping/output/google_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>date</th>\n",
       "      <th>rate</th>\n",
       "      <th>review_text</th>\n",
       "      <th>object_name</th>\n",
       "      <th>object_address</th>\n",
       "      <th>overall_rating</th>\n",
       "      <th>review_num</th>\n",
       "      <th>object_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hasan TATLI</td>\n",
       "      <td>2 months ago</td>\n",
       "      <td>1</td>\n",
       "      <td>Pas de formation possible\\nPas de financement ...</td>\n",
       "      <td>Pôle emploi</td>\n",
       "      <td>5 rue de quimper 68100 mulhouse</td>\n",
       "      <td>2.7</td>\n",
       "      <td>32</td>\n",
       "      <td>https://www.google.com/maps/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mélissa koch</td>\n",
       "      <td>a month ago</td>\n",
       "      <td>1</td>\n",
       "      <td>Aucun respect, j’explique pourtant bien mon so...</td>\n",
       "      <td>Pôle emploi</td>\n",
       "      <td>5 rue de quimper 68100 mulhouse</td>\n",
       "      <td>2.7</td>\n",
       "      <td>32</td>\n",
       "      <td>https://www.google.com/maps/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ibeno albadiia</td>\n",
       "      <td>4 months ago</td>\n",
       "      <td>1</td>\n",
       "      <td>Les fonctionnaires incompétents, arrogants et ...</td>\n",
       "      <td>Pôle emploi</td>\n",
       "      <td>5 rue de quimper 68100 mulhouse</td>\n",
       "      <td>2.7</td>\n",
       "      <td>32</td>\n",
       "      <td>https://www.google.com/maps/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Elsa Queen</td>\n",
       "      <td>9 months ago</td>\n",
       "      <td>1</td>\n",
       "      <td>Aucun respect, j’explique pourtant bien mon so...</td>\n",
       "      <td>Pôle emploi</td>\n",
       "      <td>5 rue de quimper 68100 mulhouse</td>\n",
       "      <td>2.7</td>\n",
       "      <td>32</td>\n",
       "      <td>https://www.google.com/maps/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sophanonn Puth</td>\n",
       "      <td>9 months ago</td>\n",
       "      <td>1</td>\n",
       "      <td>Si je pouvais ne pas mettre d'étoiles je n'en ...</td>\n",
       "      <td>Pôle emploi</td>\n",
       "      <td>5 rue de quimper 68100 mulhouse</td>\n",
       "      <td>2.7</td>\n",
       "      <td>32</td>\n",
       "      <td>https://www.google.com/maps/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16640</th>\n",
       "      <td>Issa Traore</td>\n",
       "      <td>3 years ago</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pôle emploi</td>\n",
       "      <td>45 av. billaud varenne 17000 la rochelle</td>\n",
       "      <td>2.1</td>\n",
       "      <td>34</td>\n",
       "      <td>https://www.google.com/maps/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16641</th>\n",
       "      <td>ju mzen</td>\n",
       "      <td>5 years ago</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pôle emploi</td>\n",
       "      <td>45 av. billaud varenne 17000 la rochelle</td>\n",
       "      <td>2.1</td>\n",
       "      <td>34</td>\n",
       "      <td>https://www.google.com/maps/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16642</th>\n",
       "      <td>Mohammad Nasiruddin</td>\n",
       "      <td>a year ago</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pôle emploi</td>\n",
       "      <td>45 av. billaud varenne 17000 la rochelle</td>\n",
       "      <td>2.1</td>\n",
       "      <td>34</td>\n",
       "      <td>https://www.google.com/maps/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16643</th>\n",
       "      <td>Orianne Mouret</td>\n",
       "      <td>7 months ago</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pôle emploi</td>\n",
       "      <td>45 av. billaud varenne 17000 la rochelle</td>\n",
       "      <td>2.1</td>\n",
       "      <td>34</td>\n",
       "      <td>https://www.google.com/maps/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16644</th>\n",
       "      <td>Pogo Gs</td>\n",
       "      <td>a week ago</td>\n",
       "      <td>1</td>\n",
       "      <td>Ces gens la sont vraiment bloqués dans leur pe...</td>\n",
       "      <td>Pôle emploi</td>\n",
       "      <td>45 av. billaud varenne 17000 la rochelle</td>\n",
       "      <td>2.1</td>\n",
       "      <td>34</td>\n",
       "      <td>https://www.google.com/maps/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16645 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      name          date  rate   \n",
       "0              Hasan TATLI  2 months ago     1  \\\n",
       "1             Mélissa koch   a month ago     1   \n",
       "2           Ibeno albadiia  4 months ago     1   \n",
       "3               Elsa Queen  9 months ago     1   \n",
       "4           Sophanonn Puth  9 months ago     1   \n",
       "...                    ...           ...   ...   \n",
       "16640          Issa Traore   3 years ago     5   \n",
       "16641              ju mzen   5 years ago     1   \n",
       "16642  Mohammad Nasiruddin    a year ago     1   \n",
       "16643       Orianne Mouret  7 months ago     1   \n",
       "16644              Pogo Gs    a week ago     1   \n",
       "\n",
       "                                             review_text  object_name   \n",
       "0      Pas de formation possible\\nPas de financement ...  Pôle emploi  \\\n",
       "1      Aucun respect, j’explique pourtant bien mon so...  Pôle emploi   \n",
       "2      Les fonctionnaires incompétents, arrogants et ...  Pôle emploi   \n",
       "3      Aucun respect, j’explique pourtant bien mon so...  Pôle emploi   \n",
       "4      Si je pouvais ne pas mettre d'étoiles je n'en ...  Pôle emploi   \n",
       "...                                                  ...          ...   \n",
       "16640                                                NaN  Pôle emploi   \n",
       "16641                                                NaN  Pôle emploi   \n",
       "16642                                                NaN  Pôle emploi   \n",
       "16643                                                NaN  Pôle emploi   \n",
       "16644  Ces gens la sont vraiment bloqués dans leur pe...  Pôle emploi   \n",
       "\n",
       "                                 object_address  overall_rating  review_num   \n",
       "0               5 rue de quimper 68100 mulhouse             2.7          32  \\\n",
       "1               5 rue de quimper 68100 mulhouse             2.7          32   \n",
       "2               5 rue de quimper 68100 mulhouse             2.7          32   \n",
       "3               5 rue de quimper 68100 mulhouse             2.7          32   \n",
       "4               5 rue de quimper 68100 mulhouse             2.7          32   \n",
       "...                                         ...             ...         ...   \n",
       "16640  45 av. billaud varenne 17000 la rochelle             2.1          34   \n",
       "16641  45 av. billaud varenne 17000 la rochelle             2.1          34   \n",
       "16642  45 av. billaud varenne 17000 la rochelle             2.1          34   \n",
       "16643  45 av. billaud varenne 17000 la rochelle             2.1          34   \n",
       "16644  45 av. billaud varenne 17000 la rochelle             2.1          34   \n",
       "\n",
       "                         object_url  \n",
       "0      https://www.google.com/maps/  \n",
       "1      https://www.google.com/maps/  \n",
       "2      https://www.google.com/maps/  \n",
       "3      https://www.google.com/maps/  \n",
       "4      https://www.google.com/maps/  \n",
       "...                             ...  \n",
       "16640  https://www.google.com/maps/  \n",
       "16641  https://www.google.com/maps/  \n",
       "16642  https://www.google.com/maps/  \n",
       "16643  https://www.google.com/maps/  \n",
       "16644  https://www.google.com/maps/  \n",
       "\n",
       "[16645 rows x 9 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['object_address'] = data['object_address'].str.lower().str.strip()\n",
    "data['object_address'] = data['object_address'].str.replace(',', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['object_address', 'rate', 'review_text', 'review_num']\n",
    "new_data = pd.read_csv('/Users/camille/repo/Hetic/projet_gouv/scraping/output/google_reviews.csv', usecols=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.to_csv('/Users/camille/repo/Hetic/projet_gouv/scraping/output/google_reviews_RGPD.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pourcentage de lieux scrappés : 74.36 %\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Charger le fichier CSV contenant les données de scraping\n",
    "data_file = '/Users/camille/repo/Hetic/projet_gouv/scraping/output/google_reviews.csv'\n",
    "df_scraped_data = pd.read_csv(data_file)\n",
    "\n",
    "# Obtenir le nombre de lieux scrappés\n",
    "nombre_lieux_scrappes = len(df_scraped_data['object_address'].unique())\n",
    "\n",
    "# Afficher le nombre de lieux scrappés\n",
    "\n",
    "# Charger le fichier CSV contenant les adresses\n",
    "addresses_file = '/Users/camille/repo/Hetic/projet_gouv/scraping/pole_emploi.csv'\n",
    "df_addresses = pd.read_csv(addresses_file)\n",
    "nombre_lieux = len(df_addresses['Adresse'].unique())\n",
    "pourcentage_scrappes = round((nombre_lieux_scrappes / nombre_lieux) * 100, 2)\n",
    "\n",
    "# Afficher le pourcentage de lieux scrappés\n",
    "print(\"Pourcentage de lieux scrappés :\", pourcentage_scrappes, \"%\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gouv_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
