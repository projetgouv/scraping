{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code scraping fonctionnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_pe = pd.read_csv('/Users/camille/repo/Hetic/projet_gouv/scraping/Data/location.csv', sep=';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10:33:19] [DEBUG] - Opening the given URL\n",
      "[10:33:20] [DEBUG] - Accepting the cookies\n",
      "[10:33:28] [DEBUG] - Object_name OK : Pôle emploi\n",
      "[10:33:28] [DEBUG] - Object_address OK : 10 Rue Brancion, 75015 Paris\n",
      "[10:33:28] [DEBUG] - Except branch\n",
      "[10:33:28] [DEBUG] - Overall_rating OK : 2.8\n",
      "[10:33:28] [DEBUG] - Review_number OK : 62\n",
      "[10:33:28] [DEBUG] - clicked to load further reviews\n",
      "[10:33:28] [DEBUG] - Scroll div OK\n",
      "[10:33:55] [DEBUG] - Source code has been parsed!\n",
      "[10:33:55] [DEBUG] - Starting to iterate through the reviews...\n",
      "[10:33:55] [DEBUG] - pole emploi AVS Placement Artistes 10 RUE BRANCION 75015 Paris is done!\n",
      "[10:33:57] [DEBUG] - Opening the given URL\n",
      "[10:33:57] [DEBUG] - Accepting the cookies\n",
      "[10:34:05] [DEBUG] - Object_name OK : Pôle Emploi\n",
      "[10:34:05] [DEBUG] - Object_address OK : 11 Rue Pelée, 75011 Paris\n",
      "[10:34:05] [DEBUG] - Except branch\n",
      "[10:34:05] [DEBUG] - Overall_rating OK : 2.6\n",
      "[10:34:05] [DEBUG] - Review_number OK : 32\n",
      "[10:34:05] [DEBUG] - clicked to load further reviews\n",
      "[10:34:05] [DEBUG] - Scroll div OK\n",
      "[10:34:22] [DEBUG] - Source code has been parsed!\n",
      "[10:34:22] [DEBUG] - Starting to iterate through the reviews...\n",
      "[10:34:22] [DEBUG] - Pole emploi Beaumarchais 11 RUE PELEE 75011 Paris is done!\n",
      "[10:34:23] [DEBUG] - Opening the given URL\n",
      "[10:34:24] [DEBUG] - Accepting the cookies\n",
      "[10:34:30] [DEBUG] - Object_name OK : Pôle Emploi - Cardinet\n",
      "[10:34:31] [DEBUG] - Object_address OK : 8 Rue Bernard Buffet, 75017 Paris\n",
      "[10:34:31] [DEBUG] - Except branch\n",
      "[10:34:31] [DEBUG] - Overall_rating OK : 2.4\n",
      "[10:34:31] [DEBUG] - Review_number OK : 77\n",
      "[10:34:31] [DEBUG] - clicked to load further reviews\n",
      "[10:34:31] [DEBUG] - Scroll div OK\n",
      "[10:35:05] [DEBUG] - Source code has been parsed!\n",
      "[10:35:05] [DEBUG] - Starting to iterate through the reviews...\n",
      "[10:35:05] [DEBUG] - Pole emploi Cardinet 8 RUE Bernard Buffet 75017 Paris is done!\n",
      "[10:35:07] [DEBUG] - Opening the given URL\n",
      "[10:35:07] [DEBUG] - Accepting the cookies\n",
      "[10:35:14] [DEBUG] - Object_name OK : Pôle emploi\n",
      "[10:35:14] [DEBUG] - Object_address OK : 44 Rue Armand Carrel, 75019 Paris\n",
      "[10:35:14] [DEBUG] - Except branch\n",
      "[10:35:14] [DEBUG] - Overall_rating OK : 2.4\n",
      "[10:35:14] [DEBUG] - Review_number OK : 47\n",
      "[10:35:14] [DEBUG] - clicked to load further reviews\n",
      "[10:35:15] [DEBUG] - Scroll div OK\n",
      "[10:35:36] [DEBUG] - Source code has been parsed!\n",
      "[10:35:36] [DEBUG] - Starting to iterate through the reviews...\n",
      "[10:35:36] [DEBUG] - pole emploi Armand Carrel 44 RUE Armand Carrel 75019 Paris is done!\n",
      "[10:35:37] [DEBUG] - Opening the given URL\n",
      "[10:35:38] [DEBUG] - Accepting the cookies\n",
      "[10:35:45] [DEBUG] - Object_name OK : Pôle emploi\n",
      "[10:35:45] [DEBUG] - Object_address OK : 60 Rue Vitruve, 75020 Paris\n",
      "[10:35:45] [DEBUG] - Except branch\n",
      "[10:35:45] [DEBUG] - Overall_rating OK : 2.9\n",
      "[10:35:45] [DEBUG] - Review_number OK : 35\n",
      "[10:35:45] [DEBUG] - clicked to load further reviews\n",
      "[10:35:45] [DEBUG] - Scroll div OK\n",
      "[10:36:04] [DEBUG] - Source code has been parsed!\n",
      "[10:36:04] [DEBUG] - Starting to iterate through the reviews...\n",
      "[10:36:05] [DEBUG] - Pole emploi Vitruve 60 RUE Vitruve 75020 Paris is done!\n",
      "[10:36:06] [DEBUG] - Opening the given URL\n",
      "[10:36:06] [DEBUG] - Accepting the cookies\n",
      "[10:36:14] [DEBUG] - Object_name OK : Pôle emploi\n",
      "[10:36:14] [DEBUG] - Object_address OK : 27 Rue Daviel, 75013 Paris\n",
      "[10:36:14] [DEBUG] - Except branch\n",
      "[10:36:14] [DEBUG] - Overall_rating OK : 2.5\n",
      "[10:36:14] [DEBUG] - Review_number OK : 82\n",
      "[10:36:14] [DEBUG] - clicked to load further reviews\n",
      "[10:36:15] [DEBUG] - Scroll div OK\n",
      "[10:36:47] [DEBUG] - Source code has been parsed!\n",
      "[10:36:47] [DEBUG] - Starting to iterate through the reviews...\n",
      "[10:36:47] [DEBUG] - Pole emploi Daviel 27 RUE Daviel 75013 Paris is done!\n",
      "[10:36:48] [DEBUG] - Opening the given URL\n",
      "[10:36:49] [DEBUG] - Accepting the cookies\n",
      "[10:36:56] [DEBUG] - Object_name OK : Pôle emploi\n",
      "[10:36:56] [DEBUG] - Object_address OK : 75 Av. Jean Jaurès, 75019 Paris\n",
      "[10:36:56] [DEBUG] - Except branch\n",
      "[10:36:56] [DEBUG] - Overall_rating OK : 3.2\n",
      "[10:36:56] [DEBUG] - Review_number OK : 25\n",
      "[10:36:56] [DEBUG] - clicked to load further reviews\n",
      "[10:36:56] [DEBUG] - Scroll div OK\n",
      "[10:37:13] [DEBUG] - Source code has been parsed!\n",
      "[10:37:13] [DEBUG] - Starting to iterate through the reviews...\n",
      "[10:37:13] [DEBUG] - Pole emploi Laumière 75 AVENUE JEAN JAURES 75019 Paris is done!\n",
      "[10:37:15] [DEBUG] - Opening the given URL\n",
      "[10:37:15] [DEBUG] - Accepting the cookies\n",
      "[10:37:24] [DEBUG] - Object_name OK : Pôle emploi\n",
      "[10:37:24] [DEBUG] - Object_address OK : 11 Rue Maurice Genevoix, 75018 Paris\n",
      "[10:37:24] [DEBUG] - Except branch\n",
      "[10:37:24] [DEBUG] - Overall_rating OK : 3.0\n",
      "[10:37:24] [DEBUG] - Review_number OK : 29\n",
      "[10:37:24] [DEBUG] - clicked to load further reviews\n",
      "[10:37:24] [DEBUG] - Scroll div OK\n",
      "[10:37:41] [DEBUG] - Source code has been parsed!\n",
      "[10:37:42] [DEBUG] - Starting to iterate through the reviews...\n",
      "[10:37:42] [DEBUG] - Pole emploi Genevoix 11 RUE Maurice Genevoix 75018 Paris is done!\n",
      "[10:37:43] [DEBUG] - Opening the given URL\n",
      "[10:37:43] [DEBUG] - Accepting the cookies\n",
      "[10:37:50] [DEBUG] - Object_name OK : Pôle emploi\n",
      "[10:37:50] [DEBUG] - Object_address OK : 3 Bd Diderot, 75012 Paris\n",
      "[10:37:50] [DEBUG] - Except branch\n",
      "[10:37:50] [DEBUG] - Overall_rating OK : 2.1\n",
      "[10:37:50] [DEBUG] - Review_number OK : 54\n",
      "[10:37:50] [DEBUG] - clicked to load further reviews\n",
      "[10:37:50] [DEBUG] - Scroll div OK\n",
      "[10:38:14] [DEBUG] - Source code has been parsed!\n",
      "[10:38:14] [DEBUG] - Starting to iterate through the reviews...\n",
      "[10:38:14] [DEBUG] - Pole emploi Diderot 3 BOULEVARD Diderot 75012 Paris is done!\n",
      "[10:38:15] [DEBUG] - Opening the given URL\n",
      "[10:38:16] [DEBUG] - Accepting the cookies\n",
      "[10:38:24] [DEBUG] - Object_name OK : Pôle emploi\n",
      "[10:38:24] [DEBUG] - Object_address OK : 4 Rue Paul Lelong, 75002 Paris\n",
      "[10:38:24] [DEBUG] - Except branch\n",
      "[10:38:24] [DEBUG] - Overall_rating OK : 2.6\n",
      "[10:38:24] [DEBUG] - Review_number OK : 27\n",
      "[10:38:24] [DEBUG] - clicked to load further reviews\n",
      "[10:38:24] [DEBUG] - Scroll div OK\n",
      "[10:38:41] [DEBUG] - Source code has been parsed!\n",
      "[10:38:41] [DEBUG] - Starting to iterate through the reviews...\n",
      "[10:38:41] [DEBUG] - Pole emploi Paul Lelong 4 RUE Paul Lelong 75002 Paris is done!\n",
      "[10:38:43] [DEBUG] - Opening the given URL\n",
      "[10:38:43] [DEBUG] - Accepting the cookies\n",
      "[10:38:51] [DEBUG] - Object_name OK : Pôle emploi\n",
      "[10:38:51] [DEBUG] - Object_address OK : 26 Rue Vicq d'Azir, 75010 Paris\n",
      "[10:38:51] [DEBUG] - Except branch\n",
      "[10:38:51] [DEBUG] - Overall_rating OK : 2.7\n",
      "[10:38:51] [DEBUG] - Review_number OK : 31\n",
      "[10:38:51] [DEBUG] - clicked to load further reviews\n",
      "[10:38:51] [DEBUG] - Scroll div OK\n",
      "[10:39:09] [DEBUG] - Source code has been parsed!\n",
      "[10:39:09] [DEBUG] - Starting to iterate through the reviews...\n",
      "[10:39:09] [DEBUG] - Pole emploi Vicq d'Azir 26 RUE Vicq d'Azir 75010 Paris is done!\n",
      "[10:39:11] [DEBUG] - Opening the given URL\n",
      "[10:39:11] [DEBUG] - Accepting the cookies\n",
      "[10:39:18] [DEBUG] - Object_name OK : Pôle emploi\n",
      "[10:39:18] [DEBUG] - Object_address OK : 10 Rue Brancion, 75015 Paris\n",
      "[10:39:18] [DEBUG] - Except branch\n",
      "[10:39:18] [DEBUG] - Overall_rating OK : 2.8\n",
      "[10:39:18] [DEBUG] - Review_number OK : 62\n",
      "[10:39:18] [DEBUG] - clicked to load further reviews\n",
      "[10:39:18] [DEBUG] - Scroll div OK\n",
      "[10:39:46] [DEBUG] - Source code has been parsed!\n",
      "[10:39:46] [DEBUG] - Starting to iterate through the reviews...\n",
      "[10:39:46] [DEBUG] - Pole emploi Brancion 10 RUE Brancion 75015 Paris is done!\n",
      "[10:39:47] [DEBUG] - Opening the given URL\n",
      "[10:39:48] [DEBUG] - Accepting the cookies\n",
      "[10:39:55] [DEBUG] - Object_name OK : Pôle emploi\n",
      "[10:39:55] [DEBUG] - Object_address OK : 78 Bd Ney, 75018 Paris\n",
      "[10:39:55] [DEBUG] - Except branch\n",
      "[10:39:55] [DEBUG] - Overall_rating OK : 2.3\n",
      "[10:39:55] [DEBUG] - Review_number OK : 54\n",
      "[10:39:55] [DEBUG] - clicked to load further reviews\n",
      "[10:39:55] [DEBUG] - Scroll div OK\n",
      "[10:40:22] [DEBUG] - Source code has been parsed!\n",
      "[10:40:23] [DEBUG] - Starting to iterate through the reviews...\n",
      "[10:40:23] [DEBUG] - Pole emploi Ney 78 BOULEVARD Ney 75018 PARIS is done!\n",
      "[10:40:24] [DEBUG] - Opening the given URL\n",
      "[10:40:24] [DEBUG] - Accepting the cookies\n",
      "[10:40:33] [DEBUG] - Object_name OK : Pôle emploi - Paris 20ème Piat\n",
      "[10:40:33] [DEBUG] - Object_address OK : 51 Rue Piat, 75020 Paris\n",
      "[10:40:33] [DEBUG] - Except branch\n",
      "[10:40:33] [DEBUG] - Overall_rating OK : 3.0\n",
      "[10:40:33] [DEBUG] - Review_number OK : 20\n",
      "[10:40:33] [DEBUG] - clicked to load further reviews\n",
      "[10:40:33] [DEBUG] - Scroll div OK\n",
      "[10:40:48] [DEBUG] - Source code has been parsed!\n",
      "[10:40:49] [DEBUG] - Starting to iterate through the reviews...\n",
      "[10:40:49] [DEBUG] - Pôle emploi - Paris 20ème Piat is done!\n",
      "[10:40:50] [DEBUG] - Opening the given URL\n",
      "[10:40:50] [DEBUG] - Accepting the cookies\n",
      "[10:40:58] [DEBUG] - Object_name OK : Pôle emploi\n",
      "[10:40:58] [DEBUG] - Object_address OK : 9 Rue Friant, 75014 Paris\n",
      "[10:40:58] [DEBUG] - Except branch\n",
      "[10:40:59] [DEBUG] - Overall_rating OK : 2.8\n",
      "[10:40:59] [DEBUG] - Review_number OK : 40\n",
      "[10:40:59] [DEBUG] - clicked to load further reviews\n",
      "[10:40:59] [DEBUG] - Scroll div OK\n",
      "[10:41:20] [DEBUG] - Source code has been parsed!\n",
      "[10:41:20] [DEBUG] - Starting to iterate through the reviews...\n",
      "[10:41:20] [DEBUG] - Pole emploi Jean Moulin 9 RUE FRIANT 75014 PARIS is done!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "\n",
    "DRIVER_PATH = r'/home/oli/Projects/Google-review-scraper/chromedriver_linux64/chromedriver'\n",
    "SAVING_PATH = '/Users/camille/repo/Hetic/projet_gouv/scraping/Data'\n",
    "\n",
    "# declaring a list, that contains the urls which we want to be scraped\n",
    "OBJECT_URLS = \"https://www.google.com/maps/\"\n",
    "\n",
    "# setting up the logging object\n",
    "logger = logging.getLogger('main')\n",
    "logging.basicConfig(\n",
    "    format='[%(asctime)s] [%(levelname)s] - %(message)s',\n",
    "    datefmt='%H:%M:%S'\n",
    ")\n",
    "\n",
    "# we can change the logging level. Use logging.DEBUG if necessary\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "def scrape_an_object(object_url, location):\n",
    "    # setting the chrome driver for selenium\n",
    "    driver = webdriver.Chrome(service=Service(DRIVER_PATH))\n",
    "\n",
    "    # opening the given URL\n",
    "    logger.debug(\"Opening the given URL\")\n",
    "    driver.get(object_url)\n",
    "\n",
    "    # accepting the cookies\n",
    "    logger.debug(\"Accepting the cookies\")\n",
    "    driver.find_element(By.CLASS_NAME, \"lssxud\").click()\n",
    "\n",
    "    # waiting some random seconds\n",
    "    time.sleep(random.uniform(4, 6))\n",
    "    select_box = driver.find_element(By.XPATH, '//*[@id=\"searchboxinput\"]')\n",
    "    select_box.send_keys(location)\n",
    "    select_box.send_keys(Keys.ENTER)\n",
    "    time.sleep(2)\n",
    "    object_name = driver.find_element(\n",
    "        By.CSS_SELECTOR,\n",
    "        'h1.DUwDvf.fontHeadlineLarge'\n",
    "    ).text\n",
    "    logger.debug(f'Object_name OK : {object_name}')\n",
    "\n",
    "    object_address = driver.find_element(\n",
    "        By.CSS_SELECTOR,\n",
    "        'div.Io6YTe.fontBodyMedium'\n",
    "    ).text\n",
    "    logger.debug(f'Object_address OK : {object_address}')\n",
    "\n",
    "    # I use CSS selectors where I can because it's more robust than XPATH\n",
    "\n",
    "    try:\n",
    "\n",
    "        overall_rating = driver.find_element(\n",
    "            By.CSS_SELECTOR,\n",
    "            'div.F7nice.mmu3tf'\n",
    "        ).text.split()[0]\n",
    "        logger.debug(f'Overall_rating OK : {overall_rating}')\n",
    "\n",
    "        review_number = driver.find_element(\n",
    "            By.CSS_SELECTOR,\n",
    "            'div.F7nice.mmu3tf'\n",
    "        ).text.replace(' ', '')\n",
    "\n",
    "        review_number = int(re.compile(r'\\d+').findall(review_number)[-1])\n",
    "        logger.debug(f'Review_number OK : {review_number}')\n",
    "\n",
    "        # click to load further reviews\n",
    "        driver.find_element(\n",
    "            By.XPATH,\n",
    "            '//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[2]/div[1]/div[1]/div[2]/div/div[1]/div[2]/span[2]/span[1]/span'\n",
    "        ).click()\n",
    "\n",
    "        logger.debug('Clicked to load further reviews')\n",
    "\n",
    "        time.sleep(random.uniform(0.1, 0.5))\n",
    "\n",
    "        # find scroll layout\n",
    "        scrollable_div = driver.find_element(\n",
    "            By.XPATH,\n",
    "            '//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[2]')\n",
    "\n",
    "        logger.debug('Scroll div OK')\n",
    "\n",
    "    except NoSuchElementException:\n",
    "\n",
    "        logger.debug('Except branch')\n",
    "\n",
    "        div_num_rating = driver.find_element(\n",
    "            By.CSS_SELECTOR,\n",
    "            'div.F7nice'\n",
    "        ).text\n",
    "        overall_rating = div_num_rating.split()[0]\n",
    "        logger.debug(f'Overall_rating OK : {overall_rating}')\n",
    "\n",
    "        review_number = int(div_num_rating.split()[1].replace('(', '').replace(')', ''))\n",
    "        logger.debug(f'Review_number OK : {review_number}')\n",
    "\n",
    "        # click on the review tab\n",
    "        driver.find_element(By.XPATH, '/html/body/div[3]/div[9]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[3]/div[1]').click()\n",
    "        logger.debug('clicked to load further reviews')\n",
    "\n",
    "        time.sleep(random.uniform(0.1, 0.5))\n",
    "\n",
    "        # find scroll layout\n",
    "        scrollable_div = driver.find_element(\n",
    "            By.XPATH,\n",
    "            '//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[3]'\n",
    "        )\n",
    "        logger.debug('Scroll div OK')\n",
    "\n",
    "    time.sleep(random.uniform(2, 4))\n",
    "\n",
    "    # scroll as many times as necessary to load all reviews\n",
    "    for _ in range(0, (round(review_number / 5 - 1) + 5)):\n",
    "        driver.execute_script(\n",
    "            'arguments[0].scrollTop = arguments[0].scrollHeight',\n",
    "            scrollable_div\n",
    "        )\n",
    "        # click on 'more' button if it appears\n",
    "        try:\n",
    "            driver.find_element(\n",
    "                By.CSS_SELECTOR,\n",
    "                'button.Aq14fc'\n",
    "            ).click()\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "\n",
    "\n",
    "        time.sleep(random.uniform(1, 2))\n",
    "\n",
    "\n",
    "    # parse the HTML with a BeautifulSoup object\n",
    "    response = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    reviews_source = response.find_all('div', class_='jJc9Ad')\n",
    "    logger.debug('Source code has been parsed!')\n",
    "\n",
    "    # closing the browser\n",
    "    driver.quit()\n",
    "\n",
    "    # storing the data in a dict\n",
    "    store_main_data = {'object_name': object_name,\n",
    "                       'object_address': object_address,\n",
    "                       'overall_rating': overall_rating,\n",
    "                       'review_num': review_number,\n",
    "                       'object_url': object_url}\n",
    "\n",
    "    return store_main_data, reviews_source\n",
    "\n",
    "\n",
    "def extract_reviews(reviews_source: list) -> list:\n",
    "    \"\"\"\n",
    "    This method processes the input HTML code and returns a list containing the reviews.\n",
    "    \"\"\"\n",
    "    review_list = []\n",
    "\n",
    "    logger.debug('Starting to iterate through the reviews...')\n",
    "    for review in reviews_source:\n",
    "        # Extract the relevant information\n",
    "        user = review.find('div', class_='d4r55').text.strip()\n",
    "        date = review.find('span', class_='rsqaWe').text.strip()\n",
    "        # Find rating elements and extract the ratings\n",
    "        rate_elements = review.find_all('span', class_='kvMYJc')\n",
    "        rate = int(rate_elements[0].get('aria-label').split()[0])\n",
    "\n",
    "        review_text = review.find('span', class_='wiI7pd')\n",
    "        review_text = '' if review_text is None else review_text.text\n",
    "\n",
    "        review_list.append({\n",
    "            'name': user,\n",
    "            'date': date,\n",
    "            'rate': rate,\n",
    "            'review_text': review_text\n",
    "        })\n",
    "\n",
    "    return review_list\n",
    "\n",
    "\n",
    "def main():\n",
    "    # creating a list to store all objects scraped\n",
    "    all_objects_data = []\n",
    "\n",
    "    # iterating through each object URL in the list\n",
    "    for location in df_pe['location']:\n",
    "        store_main_data, reviews_source = scrape_an_object(OBJECT_URLS, location)\n",
    "        review_list = extract_reviews(reviews_source)\n",
    "        for review in review_list:\n",
    "            review.update(store_main_data)  # Add the common data to each review\n",
    "        all_objects_data.extend(review_list)\n",
    "        logger.debug(f'{location} is done!')\n",
    "\n",
    "    # creating a dataframe from the list of dictionaries\n",
    "    df = pd.DataFrame(all_objects_data)\n",
    "\n",
    "    # rearranging the columns\n",
    "    df = df[['name', 'date', 'rate', 'review_text', 'object_name', 'object_address', 'overall_rating', 'review_num', 'object_url']]\n",
    "\n",
    "    # writing the dataframe to a csv file\n",
    "    df.to_csv(os.path.join(SAVING_PATH, 'google_reviews.csv'), index=False)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gouv_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
