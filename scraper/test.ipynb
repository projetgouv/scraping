{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les imports des bibliothÃ¨ques\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemins et urls\n",
    "\n",
    "DRIVER_PATH = r'C:\\Users\\lucie\\OneDrive\\Documents\\dependance\\chromedriver_win32\\chromedriver'\n",
    "SAVING_PATH = r'C:\\Users\\lucie\\OneDrive\\Documents\\dependance\\scraping'\n",
    "\n",
    "OBJECT_URLS = [ #\"https://www.google.com/maps/place/Rectorat+de+l'acad%C3%A9mie+de+Lille/@50.6261288,3.0786046,17z/data=!3m1!4b1!4m6!3m5!1s0x47c32a77b52d7745:0x6571dfb526d328d9!8m2!3d50.6261254!4d3.0811795!16s%2Fg%2F1tvq38jm\",\n",
    "               \"https://www.google.com/maps/\"\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging\n",
    "\n",
    "logger = logging.getLogger('main')\n",
    "logging.basicConfig(\n",
    "    format='[%(asctime)s] [%(levelname)s] - %(message)s',\n",
    "    datefmt='%H:%M:%S'\n",
    "    )\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le scraper sans location\n",
    "\n",
    "def scrape_an_object(object_url: str, location: str) -> tuple :\n",
    "\n",
    "    # setting the chrome driver for selenium\n",
    "    driver = webdriver.Chrome(service=Service(DRIVER_PATH))\n",
    "\n",
    "    # opening the given URL\n",
    "    logger.debug(\"Opening the given URL\")\n",
    "    driver.get(object_url)\n",
    "    \n",
    "\n",
    "    # accepting the cookies\n",
    "    logger.debug(\"Accepting the cookies\")\n",
    "    driver.find_element(By.CLASS_NAME,\"lssxud\").click()\n",
    "\n",
    "    # writting the location in the search box\n",
    "    time.sleep(2) \n",
    "    select_box = driver.find_element(By.XPATH, '//*[@id=\"searchboxinput\"]')\n",
    "    select_box.send_keys(location)\n",
    "    select_box.send_keys(Keys.ENTER)\n",
    "\n",
    "    # waiting some random seconds\n",
    "    time.sleep(random.uniform(4,6))\n",
    "\n",
    "    # CSS selectors\n",
    "    object_name = driver.find_element(\n",
    "        By.CSS_SELECTOR,\n",
    "        'h1.DUwDvf.fontHeadlineLarge'\n",
    "    ).text\n",
    "    logger.debug(f'Object_name OK : {object_name}')\n",
    "\n",
    "    object_address = driver.find_element(\n",
    "        By.CSS_SELECTOR,\n",
    "        'div.Io6YTe.fontBodyMedium'\n",
    "    ).text\n",
    "    logger.debug(f'Object_address OK : {object_address}')\n",
    "\n",
    "\n",
    "    # for some reason sometimes google full randomly loads the page\n",
    "    # with a slightly different page structure. to be able to handle this,\n",
    "    # I created an except branch that scrapes the right objects in that scenario\n",
    "    try:\n",
    "\n",
    "        overall_rating = driver.find_element(\n",
    "            By.CSS_SELECTOR,\n",
    "            'div.F7nice.mmu3tf'\n",
    "        ).text.split()[0]\n",
    "        logger.debug(f'Overall_rating OK : {overall_rating}')\n",
    "\n",
    "        review_number = driver.find_element(\n",
    "            By.CSS_SELECTOR,\n",
    "            'div.F7nice.mmu3tf'\n",
    "        ).text.replace(' ','')\n",
    "\n",
    "        review_number = int(re.compile(r'\\d+').findall(review_number)[-1])\n",
    "        logger.debug(f'Review_number OK : {review_number}')\n",
    "\n",
    "        # click to load further reviews\n",
    "        driver.find_element(\n",
    "            By.XPATH,\n",
    "            '//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[2]/div[1]/div[1]/div[2]/div/div[1]/div[2]/span[2]/span[1]/span'\n",
    "        ).click()\n",
    "\n",
    "        logger.debug('Clicked to load further reviews')\n",
    "    \n",
    "        time.sleep(random.uniform(0.1, 0.5))\n",
    "\n",
    "        # find scroll layout\n",
    "        scrollable_div = driver.find_element(\n",
    "            By.XPATH,\n",
    "            '//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[2]'\n",
    "        )\n",
    "\n",
    "        logger.debug('Scroll div OK')\n",
    "     \n",
    "    except NoSuchElementException:\n",
    "\n",
    "        logger.debug('Except branch')\n",
    "\n",
    "        div_num_rating = driver.find_element(\n",
    "            By.CSS_SELECTOR,\n",
    "            'div.F7nice'\n",
    "        ).text\n",
    "        overall_rating = div_num_rating.split()[0]\n",
    "        logger.debug(f'Overall_rating OK : {overall_rating}')\n",
    "\n",
    "        review_number = int(div_num_rating.split()[1].replace('(','').replace(')',''))\n",
    "        logger.debug(f'Review_number OK : {review_number}')\n",
    "\n",
    "        # click on the review tab\n",
    "        driver.find_element(By.XPATH,'/html/body/div[3]/div[9]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[3]/div[1]').click()\n",
    "        logger.debug('clicked to load further reviews')\n",
    "\n",
    "        time.sleep(random.uniform(0.1, 0.5))\n",
    "\n",
    "        # find scroll layout\n",
    "        scrollable_div = driver.find_element(\n",
    "            By.XPATH,\n",
    "            '//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[3]'\n",
    "        )\n",
    "\n",
    "        logger.debug('Scroll div OK')\n",
    "\n",
    "    time.sleep(random.uniform(2,4))\n",
    "\n",
    "    # button lire plus\n",
    "    button_lire_plus = driver.find_elements(By.CLASS_NAME,'w8nwRe.kyuRq')\n",
    "    for i in button_lire_plus:\n",
    "        i.click()\n",
    "\n",
    "    # scroll as many times as necessary to load all reviews\n",
    "    for _ in range(0,(round(review_number/5 - 1)+1)):\n",
    "        driver.execute_script(\n",
    "            'arguments[0].scrollTop = arguments[0].scrollHeight',\n",
    "            scrollable_div\n",
    "        )\n",
    "        time.sleep(random.uniform(1, 2))\n",
    "\n",
    "    # parse the html with a bs object\n",
    "    response = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    reviews_source = response.find_all('div', class_='jJc9Ad')\n",
    "    logger.debug('Source code has been parsed!')\n",
    "\n",
    "    # closing the browser\n",
    "    driver.close()\n",
    "\n",
    "    # storing the data in a dict\n",
    "    store_main_data = {'object_name': object_name,\n",
    "                       'object_address': object_address,\n",
    "                       'overall_rating': overall_rating,\n",
    "                       'review_num': review_number,\n",
    "                       'object_url':object_url}\n",
    "\n",
    "    return store_main_data, reviews_source\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_reviews(reviews_source: list) -> list:\n",
    "\n",
    "    r\"\"\"\n",
    "    This method processes the input html code and returns a list \n",
    "    containing the reviews.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    review_list = []\n",
    "\n",
    "    logger.debug('Starting iterate trough the reviews...')\n",
    "    for review in reviews_source:\n",
    "\n",
    "        # extract the relevant informations\n",
    "        user = review.find('div', class_= 'd4r55').text.strip()\n",
    "        date = review.find('span', class_= 'rsqaWe').text.strip()\n",
    "        rate = len(review.find('span',class_ = 'kvMYJc'))\n",
    "        review_text = review.find('span', class_= 'wiI7pd')\n",
    "        review_text = '' if review_text is None else review_text.text \n",
    "        reply_source = review.find('div', class_= 'CDe7pd')\n",
    "        reply = reply_source.text if reply_source else '-'\n",
    "\n",
    "\n",
    "        review_list.append({'name': user,\n",
    "                            'date': date,\n",
    "                            'rate': rate,\n",
    "                            'review_text': review_text,\n",
    "                            'reply': reply})\n",
    "\n",
    "    return review_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 58\u001b[0m\n\u001b[0;32m     56\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m'\u001b[39m\u001b[39mFinished!\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     57\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m---> 58\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[9], line 8\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m i, url \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(OBJECT_URLS):\n\u001b[0;32m      7\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m----> 8\u001b[0m         time\u001b[39m.\u001b[39;49msleep(random\u001b[39m.\u001b[39;49muniform(\u001b[39m3\u001b[39;49m,\u001b[39m10\u001b[39;49m))\n\u001b[0;32m     10\u001b[0m         store_main_data, reviews_source \u001b[39m=\u001b[39m scrape_an_object(url,)\n\u001b[0;32m     11\u001b[0m         scraped_data\u001b[39m.\u001b[39mappend(store_main_data)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "\n",
    "    scraped_data =  []\n",
    "\n",
    "    # loop trough the urls and calling the necessary functions to populate the empty scraped_data list\n",
    "    for i, url in enumerate(OBJECT_URLS):\n",
    "        try:\n",
    "            time.sleep(random.uniform(3,10))\n",
    "            \n",
    "            store_main_data, reviews_source = scrape_an_object(url,)\n",
    "            scraped_data.append(store_main_data)\n",
    "\n",
    "            review_list = extract_reviews(reviews_source)\n",
    "            scraped_data[i]['reviews'] = review_list\n",
    "            print (scraped_data[i]['review_num'], len(scraped_data[i]['reviews']))\n",
    "\n",
    "            if scraped_data[i]['review_num'] != len(scraped_data[i]['reviews']):\n",
    "                logger.warning(f'For some reason not all the reviews had been scraped for the following object: {store_main_data[\"object_name\"]}')\n",
    "\n",
    "\n",
    "        except Exception as exception:\n",
    "            logger.error(f'{url} \\n {exception}')\n",
    "            scraped_data.append(\n",
    "                    {'object_name': 'Error',\n",
    "                    'object_address': 'Error',\n",
    "                    'overall_rating': 'None',\n",
    "                    'review_num': 'None',\n",
    "                    'object_url':url,\n",
    "                    'reviews':[{}]\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        logger.info(f' {i+1} URL has been finished from the total of {len(OBJECT_URLS)}')\n",
    "\n",
    "\n",
    "    # reading the dict with pandas\n",
    "    result_df = pd.json_normalize(\n",
    "                scraped_data,\n",
    "                record_path = ['reviews'],\n",
    "                errors='ignore',\n",
    "                meta=['object_name', 'object_address', 'overall_rating', 'review_num', 'object_url']\n",
    "                )\n",
    "\n",
    "\n",
    "    # reorder the columns\n",
    "\n",
    "\n",
    "    # Saving the result into an excel file\n",
    "    save_path = os.path.join(SAVING_PATH,'google_result.csv')\n",
    "    result_df.to_csv(\n",
    "        save_path,\n",
    "        index= False\n",
    "    )\n",
    "\n",
    "    logger.info(f'Successfully exported the result file in the following folder: {os.path.join(SAVING_PATH,\"google_result.csv\")}')\n",
    "    logger.info('Finished!')\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
