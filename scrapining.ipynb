{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv('Data/rec.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144 Rue de Bavay, Réseau des rectorats et DSDEN, Rectorat, RECTORAT DE L'ACADEMIE DE LILLE, LILLE\n"
     ]
    },
    {
     "ename": "NoSuchElementException",
     "evalue": "Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[3]/div/div/button[2]/div[2]/div[2]\"}\n  (Session info: chrome=113.0.5672.63)\nStacktrace:\n0   chromedriver                        0x000000010087f8ac chromedriver + 4257964\n1   chromedriver                        0x0000000100877f40 chromedriver + 4226880\n2   chromedriver                        0x00000001004b49d4 chromedriver + 281044\n3   chromedriver                        0x00000001004efa34 chromedriver + 522804\n4   chromedriver                        0x00000001004e51cc chromedriver + 479692\n5   chromedriver                        0x00000001005267e4 chromedriver + 747492\n6   chromedriver                        0x00000001004e398c chromedriver + 473484\n7   chromedriver                        0x00000001004e498c chromedriver + 477580\n8   chromedriver                        0x000000010083e900 chromedriver + 3991808\n9   chromedriver                        0x0000000100842354 chromedriver + 4006740\n10  chromedriver                        0x0000000100842940 chromedriver + 4008256\n11  chromedriver                        0x000000010084833c chromedriver + 4031292\n12  chromedriver                        0x0000000100842f34 chromedriver + 4009780\n13  chromedriver                        0x000000010081b490 chromedriver + 3847312\n14  chromedriver                        0x00000001008609f4 chromedriver + 4131316\n15  chromedriver                        0x0000000100860b4c chromedriver + 4131660\n16  chromedriver                        0x0000000100871230 chromedriver + 4198960\n17  libsystem_pthread.dylib             0x000000019735606c _pthread_start + 148\n18  libsystem_pthread.dylib             0x0000000197350e2c thread_start + 8\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m location \u001b[39m=\u001b[39m row[\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mGM_search\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     50\u001b[0m \u001b[39mprint\u001b[39m(location)\n\u001b[0;32m---> 51\u001b[0m maps\u001b[39m.\u001b[39;49msearch_location(location)\n\u001b[1;32m     52\u001b[0m text_info \u001b[39m=\u001b[39m TextInformation(maps\u001b[39m.\u001b[39mdriver)\u001b[39m.\u001b[39mget_name()\n\u001b[1;32m     53\u001b[0m \u001b[39mprint\u001b[39m(text_info)\n",
      "Cell \u001b[0;32mIn[82], line 23\u001b[0m, in \u001b[0;36mGoogleMaps.search_location\u001b[0;34m(self, location)\u001b[0m\n\u001b[1;32m     21\u001b[0m search_box\u001b[39m.\u001b[39msend_keys(Keys\u001b[39m.\u001b[39mENTER)\n\u001b[1;32m     22\u001b[0m time\u001b[39m.\u001b[39msleep(\u001b[39m2\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m search_box\u001b[39m.\u001b[39;49mfind_element(By\u001b[39m.\u001b[39;49mXPATH,\u001b[39m'\u001b[39;49m\u001b[39m//*[@id=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mQA0Szd\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m]/div/div/div[1]/div[2]/div/div[1]/div/div/div[3]/div/div/button[2]/div[2]/div[2]\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39mclick()\n\u001b[1;32m     24\u001b[0m search_box\u001b[39m.\u001b[39mfind_element(By\u001b[39m.\u001b[39mXPATH,\u001b[39m'\u001b[39m\u001b[39m/html/body/div[3]/div[9]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[3]/div[1]\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mclick()\n\u001b[1;32m     26\u001b[0m time\u001b[39m.\u001b[39msleep(\u001b[39m4\u001b[39m)\n",
      "File \u001b[0;32m~/repo/Hetic/projet_gouv/scraping/gouv_env/lib/python3.10/site-packages/selenium/webdriver/remote/webelement.py:425\u001b[0m, in \u001b[0;36mWebElement.find_element\u001b[0;34m(self, by, value)\u001b[0m\n\u001b[1;32m    422\u001b[0m     by \u001b[39m=\u001b[39m By\u001b[39m.\u001b[39mCSS_SELECTOR\n\u001b[1;32m    423\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m[name=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 425\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute(Command\u001b[39m.\u001b[39;49mFIND_CHILD_ELEMENT, {\u001b[39m\"\u001b[39;49m\u001b[39musing\u001b[39;49m\u001b[39m\"\u001b[39;49m: by, \u001b[39m\"\u001b[39;49m\u001b[39mvalue\u001b[39;49m\u001b[39m\"\u001b[39;49m: value})[\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/repo/Hetic/projet_gouv/scraping/gouv_env/lib/python3.10/site-packages/selenium/webdriver/remote/webelement.py:403\u001b[0m, in \u001b[0;36mWebElement._execute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    401\u001b[0m     params \u001b[39m=\u001b[39m {}\n\u001b[1;32m    402\u001b[0m params[\u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_id\n\u001b[0;32m--> 403\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parent\u001b[39m.\u001b[39;49mexecute(command, params)\n",
      "File \u001b[0;32m~/repo/Hetic/projet_gouv/scraping/gouv_env/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py:440\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    438\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_executor\u001b[39m.\u001b[39mexecute(driver_command, params)\n\u001b[1;32m    439\u001b[0m \u001b[39mif\u001b[39;00m response:\n\u001b[0;32m--> 440\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror_handler\u001b[39m.\u001b[39;49mcheck_response(response)\n\u001b[1;32m    441\u001b[0m     response[\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unwrap_value(response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    442\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/repo/Hetic/projet_gouv/scraping/gouv_env/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py:245\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    243\u001b[0m         alert_text \u001b[39m=\u001b[39m value[\u001b[39m\"\u001b[39m\u001b[39malert\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    244\u001b[0m     \u001b[39mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[39m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[0;32m--> 245\u001b[0m \u001b[39mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[0;31mNoSuchElementException\u001b[0m: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[3]/div/div/button[2]/div[2]/div[2]\"}\n  (Session info: chrome=113.0.5672.63)\nStacktrace:\n0   chromedriver                        0x000000010087f8ac chromedriver + 4257964\n1   chromedriver                        0x0000000100877f40 chromedriver + 4226880\n2   chromedriver                        0x00000001004b49d4 chromedriver + 281044\n3   chromedriver                        0x00000001004efa34 chromedriver + 522804\n4   chromedriver                        0x00000001004e51cc chromedriver + 479692\n5   chromedriver                        0x00000001005267e4 chromedriver + 747492\n6   chromedriver                        0x00000001004e398c chromedriver + 473484\n7   chromedriver                        0x00000001004e498c chromedriver + 477580\n8   chromedriver                        0x000000010083e900 chromedriver + 3991808\n9   chromedriver                        0x0000000100842354 chromedriver + 4006740\n10  chromedriver                        0x0000000100842940 chromedriver + 4008256\n11  chromedriver                        0x000000010084833c chromedriver + 4031292\n12  chromedriver                        0x0000000100842f34 chromedriver + 4009780\n13  chromedriver                        0x000000010081b490 chromedriver + 3847312\n14  chromedriver                        0x00000001008609f4 chromedriver + 4131316\n15  chromedriver                        0x0000000100860b4c chromedriver + 4131660\n16  chromedriver                        0x0000000100871230 chromedriver + 4198960\n17  libsystem_pthread.dylib             0x000000019735606c _pthread_start + 148\n18  libsystem_pthread.dylib             0x0000000197350e2c thread_start + 8\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "class GoogleMaps:\n",
    "    def __init__(self):\n",
    "        # Remplacez le chemin vers le driver de votre navigateur\n",
    "        driver_path = 'chromedriver'\n",
    "        service = Service(executable_path=driver_path)\n",
    "        self.driver = webdriver.Chrome(service=service)\n",
    "\n",
    "    def search_location(self, location):\n",
    "        # Ouvrir Google Maps\n",
    "        self.driver.get('https://www.google.com/maps/')\n",
    "        self.driver.maximize_window()\n",
    "        skip = self.driver.find_element(By.XPATH, '//*[@id=\"yDmH0d\"]/c-wiz/div/div/div/div[2]/div[1]/div[3]/div[1]/div[1]/form[1]/div/div/button/span').click()\n",
    "        search_box = self.driver.find_element(By.XPATH, '//*[@id=\"searchboxinput\"]')\n",
    "        search_box.send_keys(location)\n",
    "        search_box.send_keys(Keys.ENTER)\n",
    "        time.sleep(2)\n",
    "        search_box.find_element(By.XPATH,'//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[3]/div/div/button[2]/div[2]/div[2]').click()\n",
    "        search_box.find_element(By.XPATH,'/html/body/div[3]/div[9]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[3]/div[1]').click()\n",
    "        \n",
    "        time.sleep(4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TextInformation:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.soup = BeautifulSoup(\n",
    "            url.page_source,\n",
    "            'html.parser',\n",
    "            from_encoding='utf-8')\n",
    "\n",
    "    def get_name(self):\n",
    "        names = []\n",
    "        for page in range(1, 200): # Itérer sur 20 pages max (200 éléments au total)\n",
    "            elements = self.soup.select(\"div.m6QErb div.MyEned\")\n",
    "            page_names = [elem.text for elem in elements]\n",
    "            names.extend(page_names) # Ajouter les noms de la page courante à la liste complète\n",
    "        return names\n",
    "\n",
    "maps = GoogleMaps()\n",
    "for row in df1.iterrows():\n",
    "    location = row[1]['GM_search']\n",
    "    print(location)\n",
    "    maps.search_location(location)\n",
    "    text_info = TextInformation(maps.driver).get_name()\n",
    "    print(text_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adresse</th>\n",
       "      <th>Intitulé du réseau</th>\n",
       "      <th>Intitulé de la typologie</th>\n",
       "      <th>Ville</th>\n",
       "      <th>Intitulé de la structure</th>\n",
       "      <th>GM_search</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>144 Rue de Bavay</td>\n",
       "      <td>Réseau des rectorats et DSDEN</td>\n",
       "      <td>Rectorat</td>\n",
       "      <td>LILLE</td>\n",
       "      <td>RECTORAT DE L'ACADEMIE DE LILLE</td>\n",
       "      <td>144 Rue de Bavay, Réseau des rectorats et DSDE...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Adresse             Intitulé du réseau Intitulé de la typologie   \n",
       "0  144 Rue de Bavay  Réseau des rectorats et DSDEN                 Rectorat  \\\n",
       "\n",
       "   Ville         Intitulé de la structure   \n",
       "0  LILLE  RECTORAT DE L'ACADEMIE DE LILLE  \\\n",
       "\n",
       "                                           GM_search  \n",
       "0  144 Rue de Bavay, Réseau des rectorats et DSDE...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:57:59] [DEBUG] - Opening the given URL\n",
      "[11:57:59] [ERROR] - https://www.google.com/maps/place/Service+des+impôts+des+particuliers16ème+Auteuil/@48.8489947,2.2671953,17z/data=!3m1!4b1!4m6!3m5!1s0x47e67aaf3f1821a7:0x547deafe6e6b69af!8m2!3d48.8489912!4d2.2697702!16s%2Fg%2F11b6dqxpd0 \n",
      " Message: invalid argument\n",
      "  (Session info: chrome=113.0.5672.63)\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x0000000102e5f8ac chromedriver + 4257964\n",
      "1   chromedriver                        0x0000000102e57f40 chromedriver + 4226880\n",
      "2   chromedriver                        0x0000000102a94854 chromedriver + 280660\n",
      "3   chromedriver                        0x0000000102a7ef70 chromedriver + 192368\n",
      "4   chromedriver                        0x0000000102a7d34c chromedriver + 185164\n",
      "5   chromedriver                        0x0000000102a7d4ec chromedriver + 185580\n",
      "6   chromedriver                        0x0000000102a96990 chromedriver + 289168\n",
      "7   chromedriver                        0x0000000102b06da8 chromedriver + 748968\n",
      "8   chromedriver                        0x0000000102b067e4 chromedriver + 747492\n",
      "9   chromedriver                        0x0000000102ac398c chromedriver + 473484\n",
      "10  chromedriver                        0x0000000102ac498c chromedriver + 477580\n",
      "11  chromedriver                        0x0000000102e1e900 chromedriver + 3991808\n",
      "12  chromedriver                        0x0000000102e22354 chromedriver + 4006740\n",
      "13  chromedriver                        0x0000000102e22940 chromedriver + 4008256\n",
      "14  chromedriver                        0x0000000102e2833c chromedriver + 4031292\n",
      "15  chromedriver                        0x0000000102e22f34 chromedriver + 4009780\n",
      "16  chromedriver                        0x0000000102dfb490 chromedriver + 3847312\n",
      "17  chromedriver                        0x0000000102e409f4 chromedriver + 4131316\n",
      "18  chromedriver                        0x0000000102e40b4c chromedriver + 4131660\n",
      "19  chromedriver                        0x0000000102e51230 chromedriver + 4198960\n",
      "20  libsystem_pthread.dylib             0x000000019735606c _pthread_start + 148\n",
      "21  libsystem_pthread.dylib             0x0000000197350e2c thread_start + 8\n",
      "\n",
      "[11:57:59] [INFO] -  1 URL has been finished from the total of 1\n",
      "[11:57:59] [INFO] - Successfully exported the result file in the following folder: /Users/camille/repo/Hetic/projet_gouv/scraping/Data/scrape_result.xlsx\n",
      "[11:57:59] [INFO] - Finished!\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adresse</th>\n",
       "      <th>Intitulé du réseau</th>\n",
       "      <th>Intitulé de la typologie</th>\n",
       "      <th>Ville</th>\n",
       "      <th>Intitulé de la structure</th>\n",
       "      <th>GM_search</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>144 Rue de Bavay</td>\n",
       "      <td>Réseau des rectorats et DSDEN</td>\n",
       "      <td>Rectorat</td>\n",
       "      <td>LILLE</td>\n",
       "      <td>RECTORAT DE L'ACADEMIE DE LILLE</td>\n",
       "      <td>144 Rue de Bavay, Réseau des rectorats et DSDE...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Adresse             Intitulé du réseau Intitulé de la typologie   \n",
       "0  144 Rue de Bavay  Réseau des rectorats et DSDEN                 Rectorat  \\\n",
       "\n",
       "   Ville         Intitulé de la structure   \n",
       "0  LILLE  RECTORAT DE L'ACADEMIE DE LILLE  \\\n",
       "\n",
       "                                           GM_search  \n",
       "0  144 Rue de Bavay, Réseau des rectorats et DSDE...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[15:26:28] [DEBUG] - Opening the given URL\n",
      "[15:26:29] [DEBUG] - Accepting the cookies\n",
      "[15:26:38] [DEBUG] - Object_name OK : Rectorat de l'académie de Lille\n",
      "[15:26:38] [DEBUG] - Object_address OK : 144 Rue de Bavay, 59000 Lille\n",
      "[15:26:38] [DEBUG] - Except branch\n",
      "[15:26:38] [DEBUG] - Overall_rating OK : 1.9\n",
      "[15:26:38] [DEBUG] - Review_number OK : 201\n",
      "[15:26:38] [DEBUG] - clicked to load further reviews\n",
      "[15:26:38] [DEBUG] - Scroll div OK\n",
      "[15:27:40] [DEBUG] - Source code has been parsed!\n",
      "[15:27:40] [DEBUG] - Starting iterate trough the reviews...\n",
      "[15:27:40] [DEBUG] - 144 Rue de Bavay, Réseau des rectorats et DSDEN, Rectorat, RECTORAT DE L'ACADEMIE DE LILLE, LILLE is done!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "\n",
    "DRIVER_PATH = r'/home/oli/Projects/Google-review-scraper/chromedriver_linux64/chromedriver'\n",
    "SAVING_PATH = '/Users/camille/repo/Hetic/projet_gouv/scraping/Data'\n",
    "\n",
    "# declaring a list, that contains the urls wich we want to be scraped\n",
    "OBJECT_URLS = \"https://www.google.com/maps/\"\n",
    "    \n",
    "\n",
    "# setting up the logging object\n",
    "logger = logging.getLogger('main')\n",
    "logging.basicConfig(\n",
    "    format='[%(asctime)s] [%(levelname)s] - %(message)s',\n",
    "    datefmt='%H:%M:%S'\n",
    "    )\n",
    "\n",
    "# we can change the logging level. Use logging.DEBUG if necesarry\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "def scrape_an_object(object_url, location):\n",
    "    # setting the chrome driver for selenium\n",
    "    driver = webdriver.Chrome(service=Service(DRIVER_PATH))\n",
    "\n",
    "    # opening the given URL\n",
    "    logger.debug(\"Opening the given URL\")\n",
    "    driver.get(object_url)\n",
    "\n",
    "    # accepting the cookies\n",
    "    logger.debug(\"Accepting the cookies\")\n",
    "    driver.find_element(By.CLASS_NAME,\"lssxud\").click()\n",
    "\n",
    "    # waiting some random seconds\n",
    "    time.sleep(random.uniform(4,6))\n",
    "    select_box = driver.find_element(By.XPATH, '//*[@id=\"searchboxinput\"]')\n",
    "    select_box.send_keys(location)\n",
    "    select_box.send_keys(Keys.ENTER)\n",
    "    time.sleep(2)\n",
    "    object_name = driver.find_element(\n",
    "    By.CSS_SELECTOR,\n",
    "    'h1.DUwDvf.fontHeadlineLarge'\n",
    "    ).text\n",
    "    logger.debug(f'Object_name OK : {object_name}')\n",
    "\n",
    "    object_address = driver.find_element(\n",
    "        By.CSS_SELECTOR,\n",
    "        'div.Io6YTe.fontBodyMedium'\n",
    "    ).text\n",
    "    logger.debug(f'Object_address OK : {object_address}')\n",
    "\n",
    "    # I use CSS selectors where I can, because its more robust than XPATH\n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "\n",
    "        overall_rating = driver.find_element(\n",
    "            By.CSS_SELECTOR,\n",
    "            'div.F7nice.mmu3tf'\n",
    "        ).text.split()[0]\n",
    "        logger.debug(f'Overall_rating OK : {overall_rating}')\n",
    "\n",
    "        review_number = driver.find_element(\n",
    "            By.CSS_SELECTOR,\n",
    "            'div.F7nice.mmu3tf'\n",
    "        ).text.replace(' ','')\n",
    "\n",
    "        review_number = int(re.compile(r'\\d+').findall(review_number)[-1])\n",
    "        logger.debug(f'Review_number OK : {review_number}')\n",
    "\n",
    "        # click to load further reviews\n",
    "        driver.find_element(\n",
    "            By.XPATH,\n",
    "            '//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[2]/div[1]/div[1]/div[2]/div/div[1]/div[2]/span[2]/span[1]/span'\n",
    "        ).click()\n",
    "\n",
    "        logger.debug('Clicked to load further reviews')\n",
    "    \n",
    "        time.sleep(random.uniform(0.1, 0.5))\n",
    "\n",
    "        # find scroll layout\n",
    "        scrollable_div = driver.find_element(\n",
    "            By.XPATH,\n",
    "            '//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[2]'\n",
    "        )\n",
    "\n",
    "        logger.debug('Scroll div OK')\n",
    "     \n",
    "    except NoSuchElementException:\n",
    "\n",
    "        logger.debug('Except branch')\n",
    "\n",
    "        div_num_rating = driver.find_element(\n",
    "            By.CSS_SELECTOR,\n",
    "            'div.F7nice'\n",
    "        ).text\n",
    "        overall_rating = div_num_rating.split()[0]\n",
    "        logger.debug(f'Overall_rating OK : {overall_rating}')\n",
    "\n",
    "        review_number = int(div_num_rating.split()[1].replace('(','').replace(')',''))\n",
    "        logger.debug(f'Review_number OK : {review_number}')\n",
    "\n",
    "        # click on the review tab\n",
    "        driver.find_element(By.XPATH,'/html/body/div[3]/div[9]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[3]/div[1]').click()\n",
    "        logger.debug('clicked to load further reviews')\n",
    "\n",
    "        time.sleep(random.uniform(0.1, 0.5))\n",
    "\n",
    "        # find scroll layout\n",
    "        scrollable_div = driver.find_element(\n",
    "            By.XPATH,\n",
    "            '//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[3]'\n",
    "        )\n",
    "        logger.debug('Scroll div OK')\n",
    "\n",
    "\n",
    "    time.sleep(random.uniform(2,4))\n",
    "\n",
    "    # scroll as many times as necessary to load all reviews\n",
    "    for _ in range(0,(round(review_number/5 - 1)+1)):\n",
    "        driver.execute_script(\n",
    "            'arguments[0].scrollTop = arguments[0].scrollHeight',\n",
    "            scrollable_div\n",
    "        )\n",
    "        time.sleep(random.uniform(1, 2))\n",
    "\n",
    "    # parse the html with a bs object\n",
    "    response = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    reviews_source = response.find_all('div', class_='jJc9Ad')\n",
    "    logger.debug('Source code has been parsed!')\n",
    "\n",
    "    # closing the browser\n",
    "    \n",
    "\n",
    "    # storing the data in a dict\n",
    "    store_main_data = {'object_name': object_name,\n",
    "                       'object_address': object_address,\n",
    "                       'overall_rating': overall_rating,\n",
    "                       'review_num': review_number,\n",
    "                       'object_url':object_url}\n",
    "\n",
    "    return store_main_data, reviews_source\n",
    "\n",
    "def extract_reviews(reviews_source: list) -> list:\n",
    "\n",
    "    r\"\"\"\n",
    "    This method processes the input html code and returns a list \n",
    "    containing the reviews.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    review_list = []\n",
    "\n",
    "    logger.debug('Starting iterate trough the reviews...')\n",
    "    for review in reviews_source:\n",
    "\n",
    "        # extract the relevant informations\n",
    "        user = review.find('div', class_= 'd4r55').text.strip()\n",
    "        date = review.find('span', class_= 'rsqaWe').text.strip()\n",
    "        rate = len(review.find('span',class_ = 'kvMYJc'))\n",
    "        review_text = review.find('span', class_= 'wiI7pd')\n",
    "        review_text = '' if review_text is None else review_text.text \n",
    "        reply_source = review.find('div', class_= 'CDe7pd')\n",
    "        reply = reply_source.text if reply_source else '-'\n",
    "\n",
    "\n",
    "        review_list.append({'name': user,\n",
    "                            'date': date,\n",
    "                            'rate': rate,\n",
    "                            'review_text': review_text,\n",
    "                            'reply': reply})\n",
    "\n",
    "    return review_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # creating a list to store all objects scraped\n",
    "    all_objects_data = []\n",
    "\n",
    "    # iterating through each object url in the list\n",
    "    for location in df1['GM_search']:\n",
    "        store_main_data, reviews_source = scrape_an_object(OBJECT_URLS, location)\n",
    "        store_main_data = extract_reviews(reviews_source)\n",
    "        all_objects_data.append(store_main_data)\n",
    "        logger.debug(f'{location} is done!')\n",
    "\n",
    "    # creating a dataframe from the list of dictionaries\n",
    "    df = pd.DataFrame(all_objects_data)\n",
    "\n",
    "    # writing the dataframe to a csv file\n",
    "    df.to_csv(os.path.join(SAVING_PATH, 'google_reviews.csv'), index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gouv_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
