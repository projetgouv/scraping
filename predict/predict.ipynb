{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "cols = ['rate', 'review_text']\n",
    "dataset = pd.read_csv(\"/Users/camille/repo/Hetic/projet_gouv/scraping/data_nlp/google_reviews_RGPD.csv\", usecols=cols)\n",
    "dataset['sentiment'] = dataset['rate'].apply(lambda x: 1 if x >= 2.5 else 0)\n",
    "dataset.dropna(subset=['review_text'], inplace=True)\n",
    "for_test = dataset.sample(frac=0.2, random_state=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['rate', 'review_text']\n",
    "dataset = pd.read_csv(\"/Users/camille/repo/Hetic/projet_gouv/scraping/data_nlp/google_reviews_RGPD.csv\", usecols=cols)\n",
    "dataset['sentiment'] = dataset['rate'].apply(lambda x: 1 if x >= 2.5 else 0)\n",
    "dataset.dropna(subset=['review_text'], inplace=True)\n",
    "for_train = dataset.sample(frac=0.8, random_state=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.dropna(subset=['review_text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['sentiment'] = dataset['rate'].apply(lambda x: 1 if x >= 2.5 else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rate</th>\n",
       "      <th>review_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13136</th>\n",
       "      <td>2</td>\n",
       "      <td>Franchement nul</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5922</th>\n",
       "      <td>5</td>\n",
       "      <td>Toujours au top, bien reçu, bon conseil et cel...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5055</th>\n",
       "      <td>1</td>\n",
       "      <td>Très mauvaise accueil thelephonique a ce jour ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9769</th>\n",
       "      <td>1</td>\n",
       "      <td>C'est une catastrophe ce pole emploi....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4277</th>\n",
       "      <td>1</td>\n",
       "      <td>Est-ce normal que le pole emploi ferme a 13h  ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2398</th>\n",
       "      <td>1</td>\n",
       "      <td>totalement incompétents, je n'ai jamais été re...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14914</th>\n",
       "      <td>1</td>\n",
       "      <td>On frôle le ridicule!\\nIncompétents, je valide...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3798</th>\n",
       "      <td>5</td>\n",
       "      <td>J'ai toujours été très bien reçue au Pôle Empl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7208</th>\n",
       "      <td>4</td>\n",
       "      <td>Sa va</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1163</th>\n",
       "      <td>4</td>\n",
       "      <td>Suite à mon dossier en Immersion professionnel...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7630 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       rate                                        review_text  sentiment\n",
       "13136     2                                    Franchement nul          0\n",
       "5922      5  Toujours au top, bien reçu, bon conseil et cel...          1\n",
       "5055      1  Très mauvaise accueil thelephonique a ce jour ...          0\n",
       "9769      1           C'est une catastrophe ce pole emploi....          0\n",
       "4277      1    Est-ce normal que le pole emploi ferme a 13h  ?          0\n",
       "...     ...                                                ...        ...\n",
       "2398      1  totalement incompétents, je n'ai jamais été re...          0\n",
       "14914     1  On frôle le ridicule!\\nIncompétents, je valide...          0\n",
       "3798      5  J'ai toujours été très bien reçue au Pôle Empl...          1\n",
       "7208      4                                              Sa va          1\n",
       "1163      4  Suite à mon dossier en Immersion professionnel...          1\n",
       "\n",
       "[7630 rows x 3 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for_train \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_train = dataset.sample(frac=0.8, random_state=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rate</th>\n",
       "      <th>review_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13136</th>\n",
       "      <td>2</td>\n",
       "      <td>Franchement nul</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5922</th>\n",
       "      <td>5</td>\n",
       "      <td>Toujours au top, bien reçu, bon conseil et cel...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5055</th>\n",
       "      <td>1</td>\n",
       "      <td>Très mauvaise accueil thelephonique a ce jour ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9769</th>\n",
       "      <td>1</td>\n",
       "      <td>C'est une catastrophe ce pole emploi....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4277</th>\n",
       "      <td>1</td>\n",
       "      <td>Est-ce normal que le pole emploi ferme a 13h  ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2398</th>\n",
       "      <td>1</td>\n",
       "      <td>totalement incompétents, je n'ai jamais été re...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14914</th>\n",
       "      <td>1</td>\n",
       "      <td>On frôle le ridicule!\\nIncompétents, je valide...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3798</th>\n",
       "      <td>5</td>\n",
       "      <td>J'ai toujours été très bien reçue au Pôle Empl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7208</th>\n",
       "      <td>4</td>\n",
       "      <td>Sa va</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1163</th>\n",
       "      <td>4</td>\n",
       "      <td>Suite à mon dossier en Immersion professionnel...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7630 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       rate                                        review_text  sentiment\n",
       "13136     2                                    Franchement nul          0\n",
       "5922      5  Toujours au top, bien reçu, bon conseil et cel...          1\n",
       "5055      1  Très mauvaise accueil thelephonique a ce jour ...          0\n",
       "9769      1           C'est une catastrophe ce pole emploi....          0\n",
       "4277      1    Est-ce normal que le pole emploi ferme a 13h  ?          0\n",
       "...     ...                                                ...        ...\n",
       "2398      1  totalement incompétents, je n'ai jamais été re...          0\n",
       "14914     1  On frôle le ridicule!\\nIncompétents, je valide...          0\n",
       "3798      5  J'ai toujours été très bien reçue au Pôle Empl...          1\n",
       "7208      4                                              Sa va          1\n",
       "1163      4  Suite à mon dossier en Immersion professionnel...          1\n",
       "\n",
       "[7630 rows x 3 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for_train  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du jeu de donnees\n",
    "from transformers import CamembertTokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "import torch\n",
    "import seaborn\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import CamembertForSequenceClassification, CamembertTokenizer, AdamW\n",
    "\n",
    "\n",
    "reviews = for_train['review_text'].values.tolist()\n",
    "sentiments = for_train['sentiment'].values.tolist()\n",
    " \n",
    "# On charge l'objet \"tokenizer\"de camemBERT qui va servir a encoder\n",
    "# 'camebert-base' est la version de camembert qu'on choisit d'utiliser\n",
    "# 'do_lower_case' à True pour qu'on passe tout en miniscule\n",
    "TOKENIZER =  AutoTokenizer.from_pretrained(\"camembert-base\")\n",
    "MAX_LENGTH = 128  \n",
    "# La fonction batch_encode_plus encode un batch de donnees\n",
    "encoded_batch = TOKENIZER.batch_encode_plus(reviews,\n",
    "                                            add_special_tokens=True,\n",
    "                                            max_length=MAX_LENGTH,\n",
    "                                            padding=True,\n",
    "                                            truncation=True,\n",
    "                                            return_attention_mask = True,\n",
    "                                            return_tensors = 'pt')\n",
    " \n",
    "# On transforme la liste des sentiments en tenseur\n",
    "sentiments = torch.tensor(sentiments)\n",
    " \n",
    "# On calcule l'indice qui va delimiter nos datasets d'entrainement et de validation\n",
    "# On utilise 80% du jeu de donnée pour l'entrainement et les 20% restant pour la validation\n",
    "split_border = int(len(sentiments)*0.8)\n",
    " \n",
    " \n",
    "train_dataset = TensorDataset(\n",
    "    encoded_batch['input_ids'][:split_border],\n",
    "    encoded_batch['attention_mask'][:split_border],\n",
    "    sentiments[:split_border])\n",
    "validation_dataset = TensorDataset(\n",
    "    encoded_batch['input_ids'][split_border:],\n",
    "    encoded_batch['attention_mask'][split_border:],\n",
    "    sentiments[split_border:])\n",
    " \n",
    " \n",
    "batch_size = 32\n",
    " \n",
    "# On cree les DataLoaders d'entrainement et de validation\n",
    "# Le dataloader est juste un objet iterable\n",
    "# On le configure pour iterer le jeu d'entrainement de façon aleatoire et creer les batchs.\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            sampler = RandomSampler(train_dataset),\n",
    "            batch_size = batch_size)\n",
    " \n",
    "validation_dataloader = DataLoader(\n",
    "            validation_dataset,\n",
    "            sampler = SequentialSampler(validation_dataset),\n",
    "            batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# On la version pre-entrainee de camemBERT 'base'\n",
    "model = CamembertForSequenceClassification.from_pretrained(\n",
    "    'camembert-base',\n",
    "    num_labels = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/camille/repo/Hetic/projet_gouv/scraping/clean_data_env/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # Learning Rate\n",
    "                  eps = 1e-8 # Epsilon\n",
    "                  )\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "\n",
    "def preprocess(raw_reviews, sentiments=None):\n",
    "    encoded_batch = TOKENIZER.batch_encode_plus(raw_reviews,\n",
    "                                                truncation=True,\n",
    "                                                pad_to_max_length=True,\n",
    "                                                return_attention_mask=True,\n",
    "                                                return_tensors='pt')\n",
    "    if sentiments:\n",
    "        sentiments = torch.tensor(sentiments)\n",
    "        return encoded_batch['input_ids'], encoded_batch['attention_mask'], sentiments\n",
    "    return encoded_batch['input_ids'], encoded_batch['attention_mask']\n",
    "\n",
    "def predict(reviews, model):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        input_ids, attention_mask = preprocess(reviews)\n",
    "        output = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = output.logits\n",
    "        return torch.argmax(logits, dim=1)\n",
    "\n",
    "def evaluate(reviews, sentiments):\n",
    "    predictions = predict(reviews, model)\n",
    "    f1_score = metrics.f1_score(sentiments, predictions, average='weighted', zero_division=0)\n",
    "    confusion_matrix = metrics.confusion_matrix(sentiments, predictions)\n",
    "    \n",
    "    # Plotting the ROC curve\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(sentiments, predictions)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Create a dataframe with predicted and sentiment columns\n",
    "    df = pd.DataFrame({'Predicted': predictions, 'Sentiment': sentiments})\n",
    "    return f1_score, confusion_matrix, df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/camille/repo/Hetic/projet_gouv/scraping/clean_data_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/camille/repo/Hetic/projet_gouv/scraping/clean_data_env/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 11.10 GB, other allocations: 6.93 GB, max allowed: 18.13 GB). Tried to allocate 120.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 158\u001b[0m\n\u001b[1;32m    156\u001b[0m classifier \u001b[39m=\u001b[39m SentimentClassifier()\n\u001b[1;32m    157\u001b[0m dataset_file \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/Users/camille/repo/Hetic/projet_gouv/scraping/data_nlp/google_reviews_RGPD.csv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 158\u001b[0m classifier\u001b[39m.\u001b[39;49mtrain(dataset_file)\n\u001b[1;32m    160\u001b[0m dataset_test \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(dataset_file)\n\u001b[1;32m    161\u001b[0m dataset_test \u001b[39m=\u001b[39m dataset_test\u001b[39m.\u001b[39msample(frac\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m)\n",
      "Cell \u001b[0;32mIn[29], line 94\u001b[0m, in \u001b[0;36mSentimentClassifier.train\u001b[0;34m(self, dataset_file)\u001b[0m\n\u001b[1;32m     91\u001b[0m sentiment \u001b[39m=\u001b[39m batch[\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     93\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 94\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m     95\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m     96\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m     97\u001b[0m     labels\u001b[39m=\u001b[39;49msentiment\n\u001b[1;32m     98\u001b[0m )\n\u001b[1;32m     99\u001b[0m logits \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlogits\n\u001b[1;32m    100\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(logits\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_labels), sentiment\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m~/repo/Hetic/projet_gouv/scraping/clean_data_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1502\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/repo/Hetic/projet_gouv/scraping/clean_data_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1509\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1510\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1512\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repo/Hetic/projet_gouv/scraping/clean_data_env/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py:1086\u001b[0m, in \u001b[0;36mCamembertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1079\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1080\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1082\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1083\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1086\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroberta(\n\u001b[1;32m   1087\u001b[0m     input_ids,\n\u001b[1;32m   1088\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1089\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1090\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1091\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1092\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1093\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1094\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1095\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1096\u001b[0m )\n\u001b[1;32m   1097\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1098\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[0;32m~/repo/Hetic/projet_gouv/scraping/clean_data_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1502\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/repo/Hetic/projet_gouv/scraping/clean_data_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1509\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1510\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1512\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repo/Hetic/projet_gouv/scraping/clean_data_env/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py:904\u001b[0m, in \u001b[0;36mCamembertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    895\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    897\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m    898\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    899\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    902\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    903\u001b[0m )\n\u001b[0;32m--> 904\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    905\u001b[0m     embedding_output,\n\u001b[1;32m    906\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m    907\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    908\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    909\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m    910\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    911\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    912\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    913\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    914\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    915\u001b[0m )\n\u001b[1;32m    916\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    917\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/repo/Hetic/projet_gouv/scraping/clean_data_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1502\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/repo/Hetic/projet_gouv/scraping/clean_data_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1509\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1510\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1512\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repo/Hetic/projet_gouv/scraping/clean_data_env/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py:541\u001b[0m, in \u001b[0;36mCamembertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    532\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    533\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    534\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    538\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    539\u001b[0m     )\n\u001b[1;32m    540\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 541\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    542\u001b[0m         hidden_states,\n\u001b[1;32m    543\u001b[0m         attention_mask,\n\u001b[1;32m    544\u001b[0m         layer_head_mask,\n\u001b[1;32m    545\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    546\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    547\u001b[0m         past_key_value,\n\u001b[1;32m    548\u001b[0m         output_attentions,\n\u001b[1;32m    549\u001b[0m     )\n\u001b[1;32m    551\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    552\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/repo/Hetic/projet_gouv/scraping/clean_data_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1502\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/repo/Hetic/projet_gouv/scraping/clean_data_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1509\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1510\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1512\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repo/Hetic/projet_gouv/scraping/clean_data_env/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py:425\u001b[0m, in \u001b[0;36mCamembertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    414\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    415\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    422\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    423\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    424\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    426\u001b[0m         hidden_states,\n\u001b[1;32m    427\u001b[0m         attention_mask,\n\u001b[1;32m    428\u001b[0m         head_mask,\n\u001b[1;32m    429\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    430\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    431\u001b[0m     )\n\u001b[1;32m    432\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    434\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/repo/Hetic/projet_gouv/scraping/clean_data_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1502\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/repo/Hetic/projet_gouv/scraping/clean_data_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1509\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1510\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1512\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repo/Hetic/projet_gouv/scraping/clean_data_env/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py:352\u001b[0m, in \u001b[0;36mCamembertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    343\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    344\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    351\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 352\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    353\u001b[0m         hidden_states,\n\u001b[1;32m    354\u001b[0m         attention_mask,\n\u001b[1;32m    355\u001b[0m         head_mask,\n\u001b[1;32m    356\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    357\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    358\u001b[0m         past_key_value,\n\u001b[1;32m    359\u001b[0m         output_attentions,\n\u001b[1;32m    360\u001b[0m     )\n\u001b[1;32m    361\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    362\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/repo/Hetic/projet_gouv/scraping/clean_data_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1502\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/repo/Hetic/projet_gouv/scraping/clean_data_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1509\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1510\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1512\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repo/Hetic/projet_gouv/scraping/clean_data_env/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py:272\u001b[0m, in \u001b[0;36mCamembertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    269\u001b[0m         relative_position_scores_key \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39meinsum(\u001b[39m\"\u001b[39m\u001b[39mbhrd,lrd->bhlr\u001b[39m\u001b[39m\"\u001b[39m, key_layer, positional_embedding)\n\u001b[1;32m    270\u001b[0m         attention_scores \u001b[39m=\u001b[39m attention_scores \u001b[39m+\u001b[39m relative_position_scores_query \u001b[39m+\u001b[39m relative_position_scores_key\n\u001b[0;32m--> 272\u001b[0m attention_scores \u001b[39m=\u001b[39m attention_scores \u001b[39m/\u001b[39;49m math\u001b[39m.\u001b[39;49msqrt(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention_head_size)\n\u001b[1;32m    273\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     \u001b[39m# Apply the attention mask is (precomputed for all layers in CamembertModel forward() function)\u001b[39;00m\n\u001b[1;32m    275\u001b[0m     attention_scores \u001b[39m=\u001b[39m attention_scores \u001b[39m+\u001b[39m attention_mask\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 11.10 GB, other allocations: 6.93 GB, max allowed: 18.13 GB). Tried to allocate 120.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import CamembertForSequenceClassification, CamembertTokenizer, AdamW\n",
    "\n",
    "\n",
    "class SentimentClassifier:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "        self.device = torch.device(\"mps\")\n",
    "        self.model = None\n",
    "    \n",
    "    def preprocess(self, raw_reviews: list, sentiments: list = None) -> tuple:\n",
    "        encoded_batch = self.tokenizer.batch_encode_plus(\n",
    "            raw_reviews,\n",
    "            truncation=True,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        ).to(self.device)\n",
    "        \n",
    "        if sentiments:\n",
    "            sentiments = torch.tensor(sentiments)\n",
    "            return encoded_batch['input_ids'], encoded_batch['attention_mask'], sentiments\n",
    "        \n",
    "        return encoded_batch['input_ids'], encoded_batch['attention_mask']\n",
    "    \n",
    "    def train(self, dataset_file: str):\n",
    "        # Load the dataset\n",
    "        cols = ['rate', 'review_text']\n",
    "        dataset = pd.read_csv(dataset_file, usecols=cols)\n",
    "        dataset['sentiment'] = dataset['rate'].apply(lambda x: 1 if x >= 2.5 else 0)\n",
    "        dataset.dropna(subset=['review_text'], inplace=True)\n",
    "        \n",
    "        # Sample the dataset\n",
    "        dataset_sample = dataset.sample(frac=0.8, random_state=200)\n",
    "        reviews_sample = dataset_sample['review_text'].values.tolist()\n",
    "        sentiments_sample = dataset_sample['sentiment'].values.tolist()\n",
    "        \n",
    "        # Encode the data\n",
    "        encoded_batch = self.preprocess(reviews_sample, sentiments_sample)\n",
    "        input_ids, attention_mask, sentiments = encoded_batch\n",
    "        \n",
    "        # Split into train and validation datasets\n",
    "        split_border = int(len(sentiments) * 0.8)\n",
    "        train_dataset = TensorDataset(\n",
    "            input_ids[:split_border],\n",
    "            attention_mask[:split_border],\n",
    "            sentiments[:split_border]\n",
    "        )\n",
    "        validation_dataset = TensorDataset(\n",
    "            input_ids[split_border:],\n",
    "            attention_mask[split_border:],\n",
    "            sentiments[split_border:]\n",
    "        )\n",
    "        \n",
    "        # Set batch size and create data loaders\n",
    "        batch_size = 10\n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            sampler=RandomSampler(train_dataset),\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        validation_dataloader = DataLoader(\n",
    "            validation_dataset,\n",
    "            sampler=SequentialSampler(validation_dataset),\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        \n",
    "        # Create the model\n",
    "        self.model = CamembertForSequenceClassification.from_pretrained(\"camembert-base\")\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        # Define the optimizer and loss function\n",
    "        optimizer = AdamW(self.model.parameters(), lr=1e-5)\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Training loop\n",
    "        epochs = 3\n",
    "        for epoch in range(epochs):\n",
    "            # Training\n",
    "            total_train_loss = 0\n",
    "            self.model.train()\n",
    "            for step, batch in enumerate(train_dataloader):\n",
    "                input_ids = batch[0].to(self.device)\n",
    "                attention_mask = batch[1].to(self.device)\n",
    "                sentiment = batch[2].to(self.device)\n",
    "                \n",
    "                self.model.zero_grad()\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=sentiment\n",
    "                )\n",
    "                logits = outputs.logits\n",
    "                loss = loss_fn(logits.view(-1, self.model.config.num_labels), sentiment.view(-1))\n",
    "                total_train_loss += loss.item()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "            print(f\"  Average training loss: {avg_train_loss:.2f}\")\n",
    "        \n",
    "        print(\"Training complete!\")\n",
    "    \n",
    "    def evaluate(self, test_reviews: list, test_sentiments: list) -> tuple:\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            input_ids, attention_mask, sentiments = self.preprocess(test_reviews, test_sentiments)\n",
    "            input_ids = input_ids.to(self.device)\n",
    "            attention_mask = attention_mask.to(self.device)\n",
    "            sentiments = sentiments.to(self.device)\n",
    "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "        \n",
    "        predictions = predictions.cpu()\n",
    "        sentiments = sentiments.cpu()\n",
    "        \n",
    "        accuracy = metrics.accuracy_score(sentiments, predictions)\n",
    "        precision = metrics.precision_score(sentiments, predictions)\n",
    "        recall = metrics.recall_score(sentiments, predictions)\n",
    "        f1_score = metrics.f1_score(sentiments, predictions)\n",
    "        confusion_matrix = metrics.confusion_matrix(sentiments, predictions)\n",
    "        \n",
    "        # Plot ROC curve\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(sentiments, predictions)\n",
    "        roc_auc = metrics.auc(fpr, tpr)\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic (ROC)')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "        \n",
    "        return accuracy, precision, recall, f1_score, confusion_matrix\n",
    "    \n",
    "    def save_model(self, filepath: str):\n",
    "        if self.model:\n",
    "            torch.save(self.model.state_dict(), filepath)\n",
    "            print(\"Model saved!\")\n",
    "        else:\n",
    "            print(\"No model to save.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    classifier = SentimentClassifier()\n",
    "    dataset_file = \"/Users/camille/repo/Hetic/projet_gouv/scraping/data_nlp/google_reviews_RGPD.csv\"\n",
    "    classifier.train(dataset_file)\n",
    "    \n",
    "    dataset_test = pd.read_csv(dataset_file)\n",
    "    dataset_test = dataset_test.sample(frac=0.2, random_state=200)\n",
    "    test_reviews = dataset_test['review_text'].values.tolist()\n",
    "    test_sentiments = dataset_test['sentiment'].values.tolist()\n",
    "    \n",
    "    accuracy, precision, recall, f1_score, confusion_matrix = classifier.evaluate(test_reviews, test_sentiments)\n",
    "    classifier.save_model(\"./sentiments.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_pos = result_pos_df['pos_info_predicted'].sum() / len(result_pos_df['pos_info_predicted'])*100\n",
    "percentage_neg_ingo = result_pos_df['neg_info_predicted'].sum() / len(result_pos_df['neg_info_predicted'])*100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clean_data_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
